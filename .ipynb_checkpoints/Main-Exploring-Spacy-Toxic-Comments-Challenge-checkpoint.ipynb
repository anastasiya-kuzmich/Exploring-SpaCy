{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b9ebeba",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Exploring-spaCy:-Toxicity-Levels\" data-toc-modified-id=\"Exploring-spaCy:-Toxicity-Levels-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Exploring spaCy: Toxicity Levels</a></span><ul class=\"toc-item\"><li><span><a href=\"#Overview\" data-toc-modified-id=\"Overview-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Overview</a></span></li><li><span><a href=\"#Dataset-Description\" data-toc-modified-id=\"Dataset-Description-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Dataset Description</a></span></li><li><span><a href=\"#File-descriptions\" data-toc-modified-id=\"File-descriptions-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>File descriptions</a></span></li></ul></li><li><span><a href=\"#Importing-packages-&amp;-dataset\" data-toc-modified-id=\"Importing-packages-&amp;-dataset-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Importing packages &amp; dataset</a></span></li><li><span><a href=\"#Exploring-the-dataset\" data-toc-modified-id=\"Exploring-the-dataset-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Exploring the dataset</a></span><ul class=\"toc-item\"><li><span><a href=\"#General-features\" data-toc-modified-id=\"General-features-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>General features</a></span></li><li><span><a href=\"#Getting-to-know-the-labels\" data-toc-modified-id=\"Getting-to-know-the-labels-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Getting to know the labels</a></span><ul class=\"toc-item\"><li><span><a href=\"#Can-a-comment-be-both-toxic-and-severely-toxic?\" data-toc-modified-id=\"Can-a-comment-be-both-toxic-and-severely-toxic?-3.2.1\"><span class=\"toc-item-num\">3.2.1&nbsp;&nbsp;</span>Can a comment be both toxic and severely toxic?</a></span></li><li><span><a href=\"#Do-the-obsene/threat/insult/identity_hate-tags-come-under-the-toxic-umbrella?\" data-toc-modified-id=\"Do-the-obsene/threat/insult/identity_hate-tags-come-under-the-toxic-umbrella?-3.2.2\"><span class=\"toc-item-num\">3.2.2&nbsp;&nbsp;</span>Do the obsene/threat/insult/identity_hate tags come under the toxic umbrella?</a></span></li><li><span><a href=\"#Adding-a-non-toxic-column:\" data-toc-modified-id=\"Adding-a-non-toxic-column:-3.2.3\"><span class=\"toc-item-num\">3.2.3&nbsp;&nbsp;</span>Adding a non-toxic column:</a></span></li><li><span><a href=\"#Removing-the-total_score-column\" data-toc-modified-id=\"Removing-the-total_score-column-3.2.4\"><span class=\"toc-item-num\">3.2.4&nbsp;&nbsp;</span>Removing the total_score column</a></span></li></ul></li></ul></li><li><span><a href=\"#Cleaning-the-comment_text-column\" data-toc-modified-id=\"Cleaning-the-comment_text-column-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Cleaning the comment_text column</a></span><ul class=\"toc-item\"><li><span><a href=\"#IP-address\" data-toc-modified-id=\"IP-address-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>IP address</a></span></li><li><span><a href=\"#Date-Time\" data-toc-modified-id=\"Date-Time-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Date Time</a></span></li><li><span><a href=\"#Formatting-characters\" data-toc-modified-id=\"Formatting-characters-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Formatting characters</a></span></li><li><span><a href=\"#Writing-functions-for-future-use-on-unseen-comments-data\" data-toc-modified-id=\"Writing-functions-for-future-use-on-unseen-comments-data-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>Writing functions for future use on unseen comments data</a></span></li></ul></li><li><span><a href=\"#Exploring-spaCy's-built-in-pre-processing-methods\" data-toc-modified-id=\"Exploring-spaCy's-built-in-pre-processing-methods-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Exploring spaCy's built-in pre-processing methods</a></span><ul class=\"toc-item\"><li><span><a href=\"#Removing-stop-words-&amp;-lemmatising\" data-toc-modified-id=\"Removing-stop-words-&amp;-lemmatising-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Removing stop-words &amp; lemmatising</a></span></li></ul></li><li><span><a href=\"#Converting-the-data-into-a-spaCy-compatible-format\" data-toc-modified-id=\"Converting-the-data-into-a-spaCy-compatible-format-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Converting the data into a spaCy-compatible format</a></span><ul class=\"toc-item\"><li><span><a href=\"#Train-test-split\" data-toc-modified-id=\"Train-test-split-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Train-test split</a></span></li></ul></li><li><span><a href=\"#Creating-a-spaCy-pipeline-to-train-a-TextCategoriser-model\" data-toc-modified-id=\"Creating-a-spaCy-pipeline-to-train-a-TextCategoriser-model-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Creating a spaCy pipeline to train a <a href=\"https://spacy.io/api/textcategorizer\" rel=\"nofollow\" target=\"_blank\">TextCategoriser</a> model</a></span><ul class=\"toc-item\"><li><span><a href=\"#Loading\" data-toc-modified-id=\"Loading-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>Loading</a></span></li><li><span><a href=\"#Adding-labels\" data-toc-modified-id=\"Adding-labels-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;</span>Adding labels</a></span></li><li><span><a href=\"#Training\" data-toc-modified-id=\"Training-7.3\"><span class=\"toc-item-num\">7.3&nbsp;&nbsp;</span>Training</a></span></li></ul></li><li><span><a href=\"#Applying-the-model-to-unseen-data\" data-toc-modified-id=\"Applying-the-model-to-unseen-data-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Applying the model to unseen data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Scoring-on-the-&quot;mini&quot;-test-set\" data-toc-modified-id=\"Scoring-on-the-&quot;mini&quot;-test-set-8.1\"><span class=\"toc-item-num\">8.1&nbsp;&nbsp;</span>Scoring on the \"mini\" test set</a></span></li><li><span><a href=\"#Scoring-on-the-larger-test-dataframe-given-by-the-classification-challenge-providers\" data-toc-modified-id=\"Scoring-on-the-larger-test-dataframe-given-by-the-classification-challenge-providers-8.2\"><span class=\"toc-item-num\">8.2&nbsp;&nbsp;</span>Scoring on the larger test dataframe given by the classification challenge providers</a></span></li></ul></li><li><span><a href=\"#Saving-the-model-to-disk\" data-toc-modified-id=\"Saving-the-model-to-disk-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Saving the model to disk</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfe19cc",
   "metadata": {},
   "source": [
    "## Exploring spaCy: Toxicity Levels "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604e2a5a",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Overview\n",
    "\n",
    "Using the SpaCy package to explore the [Toxic Comment Classification Challenge](https://www.kaggle.com/competitions/jigsaw-toxic-comment-classification-challenge/overview). \n",
    "\n",
    "- Challenge: build a model that‚Äôs capable of detecting different types of of toxicity like **threats, obscenity, insults, and identity-based hate**. \n",
    "- The dataset contains comments from Wikipedia‚Äôs talk page edits. \n",
    "- The goal is to help online discussion become more productive and respectful.\n",
    "\n",
    "### Dataset Description\n",
    "\n",
    "A large number of Wikipedia comments which have been labeled by human raters for toxic behavior. \n",
    "The types of toxicity are:\n",
    "\n",
    "- toxic\n",
    "- severe_toxic\n",
    "- obscene\n",
    "- threat\n",
    "- insult\n",
    "- identity_hate\n",
    "\n",
    "You must create a model which predicts a **probability of each type of toxicity** for each comment.\n",
    "\n",
    "### File descriptions\n",
    "\n",
    "- train.csv - the training set, contains comments with their binary labels\n",
    "- test.csv - the test set, you must predict the toxicity probabilities for these comments.\n",
    "- sample_submission.csv - a sample submission file in the correct format\n",
    "- test_labels.csv - labels for the test data; value of -1 indicates it was not used for scoring; (Note: file added after competition close!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bef354",
   "metadata": {},
   "source": [
    "## Importing packages & dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dffff7c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-22T23:07:27.582957Z",
     "start_time": "2022-12-22T23:07:26.400464Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "np.set_printoptions(precision=4)\n",
    "sns.set(font_scale=1.5)\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a82cb910",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-22T23:07:28.420307Z",
     "start_time": "2022-12-22T23:07:27.585551Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset contains 159571 entries with 8 features.\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"/Users/anastasiakuzmich/Desktop/jigsaw-toxic-comment-classification-challenge/train.csv\")\n",
    "\n",
    "print(\"The dataset contains %s entries with %s features.\" \n",
    "      % (df.shape[0], df.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d366e9",
   "metadata": {},
   "source": [
    "## Exploring the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ccc41c",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### General features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "257b9473",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-22T23:07:28.494257Z",
     "start_time": "2022-12-22T23:07:28.422284Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159571 entries, 0 to 159570\n",
      "Data columns (total 8 columns):\n",
      " #   Column         Non-Null Count   Dtype \n",
      "---  ------         --------------   ----- \n",
      " 0   id             159571 non-null  object\n",
      " 1   comment_text   159571 non-null  object\n",
      " 2   toxic          159571 non-null  int64 \n",
      " 3   severe_toxic   159571 non-null  int64 \n",
      " 4   obscene        159571 non-null  int64 \n",
      " 5   threat         159571 non-null  int64 \n",
      " 6   insult         159571 non-null  int64 \n",
      " 7   identity_hate  159571 non-null  int64 \n",
      "dtypes: int64(6), object(2)\n",
      "memory usage: 9.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9129a454",
   "metadata": {
    "hidden": true
   },
   "source": [
    "‚úèÔ∏è Okay, there's no missing values & the data types are fine. Within this dataset there is:\n",
    "\n",
    "1. An id column which might just be easier to drop. \n",
    "2. The comment_text column which contains the content of the comment itself\n",
    "3. The columns indicating the comment's annotation as toxic, severe_toxic, obscene, threat, insult or identity hate. Let's explore these further:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aba66928",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-22T23:07:28.539696Z",
     "start_time": "2022-12-22T23:07:28.498273Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.095844</td>\n",
       "      <td>0.009996</td>\n",
       "      <td>0.052948</td>\n",
       "      <td>0.002996</td>\n",
       "      <td>0.049364</td>\n",
       "      <td>0.008805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.294379</td>\n",
       "      <td>0.099477</td>\n",
       "      <td>0.223931</td>\n",
       "      <td>0.054650</td>\n",
       "      <td>0.216627</td>\n",
       "      <td>0.093420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               toxic   severe_toxic        obscene         threat  \\\n",
       "count  159571.000000  159571.000000  159571.000000  159571.000000   \n",
       "mean        0.095844       0.009996       0.052948       0.002996   \n",
       "std         0.294379       0.099477       0.223931       0.054650   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.000000       0.000000       0.000000       0.000000   \n",
       "50%         0.000000       0.000000       0.000000       0.000000   \n",
       "75%         0.000000       0.000000       0.000000       0.000000   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "              insult  identity_hate  \n",
       "count  159571.000000  159571.000000  \n",
       "mean        0.049364       0.008805  \n",
       "std         0.216627       0.093420  \n",
       "min         0.000000       0.000000  \n",
       "25%         0.000000       0.000000  \n",
       "50%         0.000000       0.000000  \n",
       "75%         0.000000       0.000000  \n",
       "max         1.000000       1.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb210a8",
   "metadata": {
    "hidden": true
   },
   "source": [
    "‚úèÔ∏è The labels are binary, so a comment is either of that label, or not of that label. 9% of the comments are toxic, 5% are obscene, 4% are insults, less than 1% are severely toxic, identity theft or a threat. \n",
    "\n",
    "Let's find out if these labels are are mutually exclusive + how many non-toxic comments there are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1262ee9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-22T23:07:28.592506Z",
     "start_time": "2022-12-22T23:07:28.541766Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.898321\n",
       "1    0.039857\n",
       "3    0.026377\n",
       "2    0.021808\n",
       "4    0.011030\n",
       "5    0.002413\n",
       "6    0.000194\n",
       "Name: total_score, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['total_score'] = (df['toxic'] \n",
    "                     + df[\"severe_toxic\"] \n",
    "                     + df[\"obscene\"] \n",
    "                     + df[\"threat\"]\n",
    "                     + df[\"insult\"]\n",
    "                     + df[\"identity_hate\"])\n",
    "\n",
    "df['total_score'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66a9906",
   "metadata": {
    "hidden": true
   },
   "source": [
    "‚úèÔ∏è My takeaways at this stage:\n",
    "\n",
    "- 90% of the comments aren't negative. That's good to know, but we are dealing with severe class imbalance...\n",
    "- A good amount of comments ticks several boxes, so the labels are not exclusive. Some tick all 6 somehow? (*How angry do you have to be?*)\n",
    "- The model needs to be capable of detecting different types of of toxicity like threats, obscenity, insults, and identity-based hate. I need to think about how I'll need to restructure the data and what I want my predictors to be.\n",
    "\n",
    "**Next questions to address:**\n",
    "\n",
    "1. Can a comment be both toxic and severely toxic? \n",
    "2. Are toxic & severe toxic the umbrella categories, then obsene/threat/insult/identity_hate their sub-categories? Restructure and see.\n",
    "3. Perhaps at this stage it is worth introducing a non-toxic column? So the model would almost predict whether the comments are toxic vs non-toxic vs severely toxic, then *if* it's toxic then what subcategory of toxic?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9d69bf",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Getting to know the labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f9c86d",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Can a comment be both toxic and severely toxic?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52acc279",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-22T23:07:28.629878Z",
     "start_time": "2022-12-22T23:07:28.594907Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of toxic comments: 15294\n",
      "Number of severely toxic comments: 1595\n",
      "Number of severely toxic and toxic comments: 1595\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of toxic comments:\", len(df[df['toxic'] == 1]))\n",
    "print(\"Number of severely toxic comments:\", len(df[df['severe_toxic'] == 1]))\n",
    "print(\"Number of severely toxic and toxic comments:\", len(df[(df['toxic'] == 1) & (df['severe_toxic'] == 1)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e4b26a",
   "metadata": {
    "hidden": true
   },
   "source": [
    "‚úèÔ∏è Yes. All severely toxic comments are also toxic, but not all toxic comments are severe. Perhaps it's worth making them exclusive within the dataframe? Could also do it later on after modelling to see if it would improve the score. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2f93f7",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Do the obsene/threat/insult/identity_hate tags come under the toxic umbrella?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12c69bcf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-22T23:07:28.641912Z",
     "start_time": "2022-12-22T23:07:28.632222Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of obsene comments that are not toxic: 523\n",
      "Number of threatening comments that are not toxic: 29\n",
      "Number of insulting comments that are not toxic: 533\n",
      "Number of identity-hateful comments that are not toxic: 103\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of obsene comments that are not toxic:\",\n",
    "      len(df[(df['toxic'] == 0) & (df['obscene'] == 1)]))\n",
    "\n",
    "print(\"Number of threatening comments that are not toxic:\",\n",
    "      len(df[(df['toxic'] == 0) & (df['threat'] == 1)]))\n",
    "\n",
    "print(\"Number of insulting comments that are not toxic:\",\n",
    "      len(df[(df['toxic'] == 0) & (df['insult'] == 1)]))\n",
    "\n",
    "print(\"Number of identity-hateful comments that are not toxic:\",\n",
    "      len(df[(df['toxic'] == 0) & (df['identity_hate'] == 1)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ff83f9",
   "metadata": {
    "hidden": true
   },
   "source": [
    "‚úèÔ∏è No, apparently they can be mutually exclusive? Which doesn't make complete sense to me. Let's inspect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b29fdb9b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-22T23:07:28.655586Z",
     "start_time": "2022-12-22T23:07:28.644724Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comment  1 :  obscene \n",
      " How do you know he is dead.  Its just his plane that crashed.  Jeezz, quit busting his nuts, folks. \n",
      "\n",
      "Comment  1 :  threat \n",
      " Please stop. If you continue to ignore our policies by introducing inappropriate pages to Wikipedia, you will be blocked. \n",
      "\n",
      "Comment  1 :  insult \n",
      " REPLY ABOVE:\n",
      "That was me, loser. The UN defines Vietnam to be part of Southeast Asia. And far as I know Vietnam is part of ASEAN, and used to be part of French Indochina with Laos and all those shit countries Anyway your culture has always been more influenced by SEA than Han Chinese (as in proper Yangtze Han Chinese, not the fringe indigenous tribes in Guangzhou/Guangxi). \n",
      "\n",
      "Just admit that you vietnamese are all a bunch of wannabe crap people. ALL the east asian people I've spoken to thinks of Vietnam as a very integral part of SEA, and we all think you're backward, dirty and speak in a horrible swearing language. Doesn't matter what crap you spout on Wikipedia, won't change the way people in the real world think. \n",
      "\n",
      "Comment  1 :  identity_hate \n",
      " Mate, sound like you are jewish\n",
      "\n",
      "Gayness is in the air \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for category in [\"obscene\", \"threat\", \"insult\", \"identity_hate\"]:\n",
    "    for i in range(1):\n",
    "        print(\"Comment \", i+1, \": \", str(category),\n",
    "              \"\\n\", \n",
    "              df[(df['toxic'] == 0) & (df[category] == 1)][\"comment_text\"].iloc[i], \n",
    "              \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9185ae09",
   "metadata": {
    "hidden": true
   },
   "source": [
    "‚úèÔ∏è Ummm... these look toxic to me? But I just re-read the challenge rules and I'm expected to classify each of the columns so I'll try not to overthink why these categories are the way they are. I will make an extra column for non-toxic, neutral comments though because in my head that will make the probabilities make sense?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe8ce10",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Adding a non-toxic column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e96b6a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-22T23:07:28.707297Z",
     "start_time": "2022-12-22T23:07:28.657227Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    143346\n",
       "0     16225\n",
       "Name: non_toxic, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def non_toxic_mapper(x):\n",
    "    if x == 0: \n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "df[\"non_toxic\"] = df[\"total_score\"].map(non_toxic_mapper)\n",
    "df[\"non_toxic\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d19c09",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Removing the total_score column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fbc640",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This column was only used to explore the data, so I'm dropping it before I start the pre-modelling stages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85407695",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-22T23:07:28.743968Z",
     "start_time": "2022-12-22T23:07:28.712601Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>non_toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  non_toxic  \n",
       "0             0        0       0       0              0          1  \n",
       "1             0        0       0       0              0          1  \n",
       "2             0        0       0       0              0          1  \n",
       "3             0        0       0       0              0          1  \n",
       "4             0        0       0       0              0          1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop(\"total_score\", axis=1)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ed98dd",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Cleaning the comment_text column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9dae9a",
   "metadata": {
    "hidden": true
   },
   "source": [
    "‚úèÔ∏è Having read up on spaCy, it appears that its pipelines work best on natural sentences because that's what its training data looks like. Extensive preprocessing, like removing stop words and lowercasing will apparently make things worse with spaCy because it uses that information for clues - [StackOverFlow](https://stackoverflow.com/a/70502883) .\n",
    "\n",
    "That said, I will still be removing newlines and formatting characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17d1aec8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-22T23:07:28.749991Z",
     "start_time": "2022-12-22T23:07:28.746190Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Explanation\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspecting examples\n",
    "\n",
    "df[\"comment_text\"].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455f9caf",
   "metadata": {
    "hidden": true
   },
   "source": [
    "‚úèÔ∏è Printing these would make the formatting characters invisible, so I ran the below in individual cells..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1caa1b78",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-22T23:07:28.754065Z",
     "start_time": "2022-12-22T23:07:28.751639Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df[\"comment_text\"].iloc[1]\n",
    "# df[\"comment_text\"].iloc[5]\n",
    "# df[\"comment_text\"].iloc[10]\n",
    "# df[\"comment_text\"].iloc[20]\n",
    "# df[\"comment_text\"].iloc[21]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999500e1",
   "metadata": {
    "hidden": true
   },
   "source": [
    "‚úèÔ∏è \n",
    "\n",
    "**To remove:**\n",
    "\n",
    "Having inspected some of the comments, I decided I will be removing the following features from the data:\n",
    "\n",
    "- IP addresses\n",
    "- Date & Time (Format: + 21:51, January 11, 2016 (UTC))\n",
    "- Newline characters (\\n) & \"\\\\\" separator\n",
    "- Non-breaking spaces (\\xa0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0fa74e",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### IP address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91e43b0f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-22T23:07:28.762303Z",
     "start_time": "2022-12-22T23:07:28.755873Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['89.205.38.27']\n",
      "['93.161.107.169']\n",
      "['70.100.229.154']\n",
      "['76.122.79.82']\n",
      "['201.215.187.159']\n",
      "['109.77.58.139', '109.76.191.188', '109.76.191.188', '109.78.224.50']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "IpAddressRegex = \"(\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3})\"\n",
    "\n",
    "for i in range(100):\n",
    "    txt = df[\"comment_text\"].iloc[i]\n",
    "    x = re.findall(IpAddressRegex, txt)\n",
    "    if len(x) > 0:\n",
    "        print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bc1307",
   "metadata": {
    "hidden": true
   },
   "source": [
    "‚úèÔ∏è The regex works, so I'll use it to clean now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a85def46",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-22T23:07:30.302760Z",
     "start_time": "2022-12-22T23:07:28.763791Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df[\"comment_text\"] = df[\"comment_text\"].replace(IpAddressRegex,'', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad4246d",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Date Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8b9f8a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-22T23:07:30.316365Z",
     "start_time": "2022-12-22T23:07:30.305703Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('21:51, January 11, 2016 (UTC)', '')]\n",
      "[('', '18:01, 16 Jun 2005 (UTC)')]\n",
      "[('28:57, August 19, 2007 (UTC)', '')]\n"
     ]
    }
   ],
   "source": [
    "DateTimeRegex = \"(\\d{2}\\:\\d{2}\\, [A-Z][a-z]{2,8}\\ \\d{1,2}\\, \\d{4}\\ \\([A-Z]{3}\\))|(\\d{2}\\:\\d{2}\\, \\d{2}\\ [A-Z][a-z]{2}\\ \\d{4}\\ \\([A-Z]{3}\\))\"\n",
    "\n",
    "for i in range(50):\n",
    "    txt = df[\"comment_text\"].iloc[i]\n",
    "    x = re.findall(DateTimeRegex, txt)\n",
    "    if len(x) > 0:\n",
    "        print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b78a46d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-22T23:07:33.390857Z",
     "start_time": "2022-12-22T23:07:30.318850Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df[\"comment_text\"] = df[\"comment_text\"].replace(DateTimeRegex,'', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89063ca3",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Checking for other pattern types:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e902a5c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-22T23:07:33.513473Z",
     "start_time": "2022-12-22T23:07:33.393919Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "459       \", 20 January 2013 (UTC)\\nThe argument at WP:N...\n",
       "594       \"\\n\\n\"\"For Security Reasons\"\" edit war\\nAs I s...\n",
       "601       All the sources needed are provided, all you n...\n",
       "637       \"40, 25 January 2008 (UTC)\\n\\nNO, i am NOT bei...\n",
       "709       \"\\nAmerican Psychologist as additional resourc...\n",
       "                                ...                        \n",
       "159090    January 2008 (UTC)\\n\\nUnlike other schools, th...\n",
       "159383    I know the images I've uploaded in the past we...\n",
       "159395    \"\\n\\n feedback as requested \\n\\nHey Antonin! I...\n",
       "159427                         for January 2006 discussion.\n",
       "159460    , 24 January 2010 (UTC)\\nWikipedia is hosted i...\n",
       "Name: comment_text, Length: 1408, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['comment_text'].str.contains('Jan')][\"comment_text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31be7846",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Patterns to add:**\n",
    "\n",
    "- 10:36, 5 January 2012\n",
    "- 27 January 2010 (UTC)\n",
    "- \"Semi-protected edit request on \"7 January 2014\n",
    "- 01:29, 30\n",
    "- January 2013 (UTC)\n",
    "\n",
    "‚úèÔ∏è The rest of the dates are mostly mentioned in context. The data should be clear after I add the below formats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cf2b929e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-22T23:07:43.979459Z",
     "start_time": "2022-12-22T23:07:33.515586Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "updated_DateTimeRegex = \"(\\d{2}\\:\\d{2}\\, [A-Z][a-z]{2,8}\\ \\d{1,2}\\, \\d{4}\\ \\([A-Z]{3}\\))|(\\d{2}\\:\\d{2}\\, \\d{2}\\ [A-Z][a-z]{2}\\ \\d{4}\\ \\([A-Z]{3}\\))|(\\d{2}\\:\\d{2}\\, \\d{1,2}\\ [A-Z][a-z]{2,8} \\d{4})|(\\d{1,2}\\ [A-Z][a-z]{2,8}\\ \\d{4}\\ \\([A-Z]{3}\\))|(\\d{1,2}\\ [A-Z][a-z]{1,7}\\ \\d{4})|(\\d{2}\\:\\d{2}\\, \\d{1,2})|([A-Z][a-z]{2,8}\\ \\d{4}\\ \\([A-Z]{3}\\))\"\n",
    "df['comment_text'] = df['comment_text'].replace(updated_DateTimeRegex,'', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d379c828",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-21T15:45:19.065168Z",
     "start_time": "2022-12-21T15:45:19.062135Z"
    },
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Formatting characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1c058e99",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-22T23:07:44.543426Z",
     "start_time": "2022-12-22T23:07:43.982416Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df['comment_text'] = df[\"comment_text\"].replace('\\\\n', ' ', regex=True)\n",
    "df['comment_text'] = df['comment_text'].replace(r'\\\\', ' ', regex=True)\n",
    "df['comment_text'] = df['comment_text'].replace(u'\\xa0', u' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f24e5b",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Writing functions for future use on unseen comments data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fffeec13",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-22T23:07:44.553426Z",
     "start_time": "2022-12-22T23:07:44.546309Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def non_toxic_mapper(x):\n",
    "    \n",
    "    \"\"\"Maps the non-toxic column based on the total score column.\"\"\"\n",
    "    \n",
    "    if x == 0: \n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def add_non_toxic_column(df):\n",
    "    \n",
    "    \"\"\"Adds a non-toxic column.\"\"\"\n",
    "    \n",
    "    df['total_score'] = (df['toxic'] + df[\"severe_toxic\"] \n",
    "                         + df[\"obscene\"] + df[\"threat\"]\n",
    "                         + df[\"insult\"] + df[\"identity_hate\"])\n",
    "\n",
    "    df[\"non_toxic\"] = df[\"total_score\"].map(non_toxic_mapper)\n",
    "    df = df.drop(\"total_score\", axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def clean_comments_column(df):\n",
    "    \n",
    "    \"\"\"Cleans the comments column.\"\"\"\n",
    "    \n",
    "    import re\n",
    "    \n",
    "    IpAddressRegex = \"(\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3})\"\n",
    "    DateTimeRegex = \"(\\d{2}\\:\\d{2}\\, [A-Z][a-z]{2,8}\\ \\d{1,2}\\, \\d{4}\\ \\([A-Z]{3}\\))|(\\d{2}\\:\\d{2}\\, \\d{2}\\ [A-Z][a-z]{2}\\ \\d{4}\\ \\([A-Z]{3}\\))|(\\d{2}\\:\\d{2}\\, \\d{1,2}\\ [A-Z][a-z]{2,8} \\d{4})|(\\d{1,2}\\ [A-Z][a-z]{2,8}\\ \\d{4}\\ \\([A-Z]{3}\\))|(\\d{1,2}\\ [A-Z][a-z]{1,7}\\ \\d{4})|(\\d{2}\\:\\d{2}\\, \\d{1,2})|([A-Z][a-z]{2,8}\\ \\d{4}\\ \\([A-Z]{3}\\))\"\n",
    "    \n",
    "    features_to_remove = [IpAddressRegex,\n",
    "                          DateTimeRegex,\n",
    "                          '\\\\n', r'\\\\']\n",
    "    \n",
    "    for feature in features_to_remove:\n",
    "        df[\"comment_text\"] = df[\"comment_text\"].replace(feature,' ', regex=True)\n",
    " \n",
    "    df['comment_text'] = df['comment_text'].replace(u'\\xa0', u' ')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598ed75e",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Exploring spaCy's built-in pre-processing methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e556d62",
   "metadata": {
    "hidden": true
   },
   "source": [
    "###  Removing stop-words & lemmatising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "da6276d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-22T23:07:44.558408Z",
     "start_time": "2022-12-22T23:07:44.556288Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# nlp = spacy.load(\"en_core_web_md\", exclude=[\"parser\", \"ner\"])\n",
    "# spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "# print('Number of stop words: %d' % len(spacy_stopwords))\n",
    "# print('First ten stop words: %s' % list(spacy_stopwords)[:10])\n",
    "# print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "83ca0f9e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-22T23:07:44.562339Z",
     "start_time": "2022-12-22T23:07:44.560316Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# comments = df[\"comment_text\"]\n",
    "# preprocessed_comments = []\n",
    "\n",
    "# for doc in tqdm(list(nlp.pipe(df[\"comment_text\"].head(100)))):\n",
    "#     preprocessed_comment = [token.lemma_ for token in doc if not token.is_stop]\n",
    "#     preprocessed_comments.append(' '.join(preprocessed_comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1b9ea3c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-22T23:07:44.566487Z",
     "start_time": "2022-12-22T23:07:44.564218Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# for i in range(3):\n",
    "#     print(df[\"comment_text\"][i])\n",
    "#     print(\" \")\n",
    "#     print(\"Processed:\", preprocessed_comments[i])\n",
    "#     print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e3266f",
   "metadata": {
    "hidden": true
   },
   "source": [
    "üí° **Note:** See the \"Supplementary-Exploring-Spacy-Toxic-Comments-With-Preprocessing\" Notebook within this repository for my experimental model used on lemmatised data with stop words excluded. \n",
    "\n",
    "Within that notebook, I kept my workflow practically identical to this one, *except* for using the hashed out code above to remove the stop words and lemmatise the contents of the comments. Having included these extra pre-processing stages, I trained the model on the resulting data achieving a 'cats_score' of 0.498, compared to 0.9+ that I get using raw sentences further down this notebook. This result is in line with what is suggested by the documentation and mentioned in StackOverFlow user discussions: spaCy does indeed work much better on natural sentences. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8572794d",
   "metadata": {},
   "source": [
    "## Converting the data into a spaCy-compatible format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c6edf0b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-22T23:07:44.573783Z",
     "start_time": "2022-12-22T23:07:44.568560Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'comment_text', 'toxic', 'severe_toxic', 'obscene', 'threat',\n",
       "       'insult', 'identity_hate', 'non_toxic'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "58d189af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-22T23:07:45.041719Z",
     "start_time": "2022-12-22T23:07:44.575781Z"
    }
   },
   "outputs": [],
   "source": [
    "# Extracting the labels\n",
    "y = df[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate', 'non_toxic']]\n",
    "\n",
    "# Labels variable to be used in the pipeline\n",
    "labels = list(y.columns)\n",
    "\n",
    "# Converting the labels to a spacy-compatible dictionary \n",
    "y = y.to_dict(\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9ce6d519",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-22T23:07:45.305277Z",
     "start_time": "2022-12-22T23:07:45.044757Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"Explanation Why the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.\", {'cats': {'toxic': 0, 'severe_toxic': 0, 'obscene': 0, 'threat': 0, 'insult': 0, 'identity_hate': 0, 'non_toxic': 1}})\n"
     ]
    }
   ],
   "source": [
    "dataset = list(zip(df['comment_text'],[{'cats': cats} for cats in y.values()]))\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2a6f9a",
   "metadata": {},
   "source": [
    "### Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b18a06ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-22T23:07:45.398561Z",
     "start_time": "2022-12-22T23:07:45.306958Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, test_data = train_test_split(dataset, \n",
    "                                         train_size=0.8, \n",
    "                                         random_state=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "67f9e32b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-22T23:07:45.417001Z",
     "start_time": "2022-12-22T23:07:45.412540Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAMPLES IN TRAINING SET:  127656\n",
      "SAMPLES IN TESTING SET:  31915\n",
      " \n",
      "TRAINING EXAMPLE: (\"Okay so no one's gonna address this? Guess that no one editing this page cares about accuracy.\", {'cats': {'toxic': 0, 'severe_toxic': 0, 'obscene': 0, 'threat': 0, 'insult': 0, 'identity_hate': 0, 'non_toxic': 1}})\n"
     ]
    }
   ],
   "source": [
    "print(\"SAMPLES IN TRAINING SET: \", len(train_data))\n",
    "print(\"SAMPLES IN TESTING SET: \", len(test_data))\n",
    "print(\" \")\n",
    "print(\"TRAINING EXAMPLE:\", train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da5c367",
   "metadata": {},
   "source": [
    "## Creating a spaCy pipeline to train a [TextCategoriser](https://spacy.io/api/textcategorizer) model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1bf8f4",
   "metadata": {},
   "source": [
    "### Loading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "49087c39",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-22T23:07:48.799942Z",
     "start_time": "2022-12-22T23:07:45.418497Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "\n",
    "nlp_train = spacy.blank(\"en\")\n",
    "nlp_train.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a2d2f0fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-22T23:07:48.829130Z",
     "start_time": "2022-12-22T23:07:48.802106Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['textcat_multilabel']\n"
     ]
    }
   ],
   "source": [
    "from spacy.pipeline.textcat_multilabel import DEFAULT_MULTI_TEXTCAT_MODEL\n",
    "\n",
    "config = {\"threshold\": 0.5, \"model\": DEFAULT_MULTI_TEXTCAT_MODEL}\n",
    "\n",
    "textcat = nlp_train.add_pipe(\"textcat_multilabel\", config=config)\n",
    "print(nlp_train.pipe_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d59f80",
   "metadata": {},
   "source": [
    "### Adding labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "26cdb136",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-22T23:07:48.837365Z",
     "start_time": "2022-12-22T23:07:48.832046Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('toxic',\n",
       " 'severe_toxic',\n",
       " 'obscene',\n",
       " 'threat',\n",
       " 'insult',\n",
       " 'identity_hate',\n",
       " 'non_toxic')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for label in labels:\n",
    "    textcat.add_label(label)\n",
    "    \n",
    "textcat.labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb5a21e",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f7bf3556",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-22T23:07:48.868439Z",
     "start_time": "2022-12-22T23:07:48.839140Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = nlp_train.begin_training()\n",
    "iterations = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "11d3f2be",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-22T23:07:48.872969Z",
     "start_time": "2022-12-22T23:07:48.870234Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "\n",
    "from spacy.util import minibatch, compounding\n",
    "from spacy.training import Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d5ae8d77",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-23T00:36:28.532665Z",
     "start_time": "2022-12-22T23:07:48.874650Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7cade73e8fb4452b91bebb98963baaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5227 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1670393c3ece4683ad4f151277a6d0cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5227 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "816dcdff547842df942597c7b7a25e66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5227 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e9bd1d4b02640a7b31b1d2960d5377b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5227 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc8f21b08eb9456fb66f3deab04bbe78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5227 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a58fd5fcef6f40b5baf97fb00e492caf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5227 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6425cf83ea14685b7765861d8cef124",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5227 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bc96277cd5948d984a9daa137b032a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5227 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7ce2926bda44c759882ab1d071c4d51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5227 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0c66b990a1547eb8963c9a78ab374a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5227 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with nlp_train.select_pipes(enable=\"textcat_multilabel\"):\n",
    "    for j in range(iterations):\n",
    "        \n",
    "        losses = {}\n",
    "        batches = minibatch(train_data, size = compounding(4.,32.,1.001))\n",
    "        pbar = tqdm(batches, total=5227)\n",
    "        \n",
    "        for batch in pbar:\n",
    "            pbar.set_description(f\"Iteration {j+1}\")\n",
    "            text, annotations = zip(*batch)\n",
    "            example = []\n",
    "            \n",
    "            for i in range(len(text)):\n",
    "                doc = nlp_train.make_doc(text[i])\n",
    "                example.append(Example.from_dict(doc, annotations[i]))\n",
    "            \n",
    "            nlp_train.update(example, sgd=optimizer, drop=0.2, losses = losses)\n",
    "\n",
    "#         print(\"\\n\\n Completed Iterations : {} \".format(j+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a4e67f",
   "metadata": {},
   "source": [
    "## Applying the model to unseen data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11eca632",
   "metadata": {},
   "source": [
    "### Scoring on the \"mini\" test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c0a8a768",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-23T00:39:33.214211Z",
     "start_time": "2022-12-23T00:39:24.719098Z"
    }
   },
   "outputs": [],
   "source": [
    "test = []\n",
    "\n",
    "for texts, annotation in test_data:\n",
    "    doc = nlp_train.make_doc(texts)\n",
    "    test.append(Example.from_dict(doc, annotation))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "798fca73",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-23T00:42:31.072983Z",
     "start_time": "2022-12-23T00:41:24.491133Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_acc': 1.0,\n",
       " 'token_p': 1.0,\n",
       " 'token_r': 1.0,\n",
       " 'token_f': 1.0,\n",
       " 'cats_score': 0.9728905931533438,\n",
       " 'cats_score_desc': 'macro AUC',\n",
       " 'cats_micro_p': 0.9387561459501423,\n",
       " 'cats_micro_r': 0.9187067364511228,\n",
       " 'cats_micro_f': 0.9286232347796408,\n",
       " 'cats_macro_p': 0.7251468206400575,\n",
       " 'cats_macro_r': 0.5551128908162205,\n",
       " 'cats_macro_f': 0.608914292378417,\n",
       " 'cats_macro_auc': 0.9728905931533438,\n",
       " 'cats_f_per_type': {'toxic': {'p': 0.7765696246930901,\n",
       "   'r': 0.7409638554216867,\n",
       "   'f': 0.7583490323685562},\n",
       "  'severe_toxic': {'p': 0.4919786096256685,\n",
       "   'r': 0.30363036303630364,\n",
       "   'f': 0.37551020408163266},\n",
       "  'obscene': {'p': 0.807061790668348,\n",
       "   'r': 0.7881773399014779,\n",
       "   'f': 0.7975077881619939},\n",
       "  'threat': {'p': 0.6764705882352942,\n",
       "   'r': 0.25555555555555554,\n",
       "   'f': 0.3709677419354839},\n",
       "  'insult': {'p': 0.7516339869281046,\n",
       "   'r': 0.612109115103127,\n",
       "   'f': 0.6747341400806747},\n",
       "  'identity_hate': {'p': 0.5979381443298969,\n",
       "   'r': 0.20938628158844766,\n",
       "   'f': 0.31016042780748665},\n",
       "  'non_toxic': {'p': 0.974375,\n",
       "   'r': 0.9759677251069454,\n",
       "   'f': 0.9751707122130906}},\n",
       " 'cats_auc_per_type': {'toxic': 0.9618181475513142,\n",
       "  'severe_toxic': 0.9850837861212415,\n",
       "  'obscene': 0.988180626982311,\n",
       "  'threat': 0.9722580082045911,\n",
       "  'insult': 0.9776397487807496,\n",
       "  'identity_hate': 0.9620705850456757,\n",
       "  'non_toxic': 0.9631832493875236},\n",
       " 'speed': 71657.19298540748}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = nlp_train.evaluate(test)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a6ee0a",
   "metadata": {},
   "source": [
    "### Scoring on the larger test dataframe given by the classification challenge providers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d852777d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-23T01:27:49.771869Z",
     "start_time": "2022-12-23T01:27:36.424948Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset contains 153164 entries with 2 features.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001cee341fdb12</td>\n",
       "      <td>Yo bitch Ja Rule is more succesful then you'll...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000247867823ef7</td>\n",
       "      <td>== From RfC ==    The title is fine as it is, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00013b17ad220c46</td>\n",
       "      <td>\"    == Sources ==    * Zawe Ashton on Lapland...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00017563c3f7919a</td>\n",
       "      <td>:If you have a look back at the source, the in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00017695ad8997eb</td>\n",
       "      <td>I don't anonymously edit articles at all.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text\n",
       "0  00001cee341fdb12  Yo bitch Ja Rule is more succesful then you'll...\n",
       "1  0000247867823ef7  == From RfC ==    The title is fine as it is, ...\n",
       "2  00013b17ad220c46  \"    == Sources ==    * Zawe Ashton on Lapland...\n",
       "3  00017563c3f7919a  :If you have a look back at the source, the in...\n",
       "4  00017695ad8997eb          I don't anonymously edit articles at all."
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = pd.read_csv(\"/Users/anastasiakuzmich/Desktop/jigsaw-toxic-comment-classification-challenge/test.csv\")\n",
    "y_test = pd.read_csv(\"/Users/anastasiakuzmich/Desktop/jigsaw-toxic-comment-classification-challenge/test_labels.csv\")\n",
    "\n",
    "print(\"The dataset contains %s entries with %s features.\" \n",
    "      % (X_test.shape[0], X_test.shape[1]))\n",
    "\n",
    "X_test = clean_comments_column(X_test)\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f512f56",
   "metadata": {},
   "source": [
    "‚úèÔ∏è It states on the competition page that the entries with values of -1 were not used for scoring (and, therefore, we do not know what the labels are). As such, I will remove these entries from the dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "34f37243",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-23T01:28:14.665353Z",
     "start_time": "2022-12-23T01:28:14.608810Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    57888\n",
       "1     6090\n",
       "Name: toxic, dtype: int64"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.concat([X_test, y_test], axis=1)\n",
    "test_df = test_df.drop(\"id\", axis=1)\n",
    "test_df = test_df[test_df[\"toxic\"] != -1]\n",
    "\n",
    "test_df[\"toxic\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "61733a27",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-23T01:28:19.304951Z",
     "start_time": "2022-12-23T01:28:19.298259Z"
    }
   },
   "outputs": [],
   "source": [
    "del X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "fc371f52",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-23T01:28:36.098958Z",
     "start_time": "2022-12-23T01:28:35.766179Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Thank you for understanding. I think very highly of you and would not revert without discussion.', {'cats': {'toxic': 0, 'severe_toxic': 0, 'obscene': 0, 'threat': 0, 'insult': 0, 'identity_hate': 0, 'non_toxic': 1}})\n"
     ]
    }
   ],
   "source": [
    "test_df['total_score'] = (test_df['toxic'] + test_df[\"severe_toxic\"] + test_df[\"obscene\"] \n",
    "                         + test_df[\"threat\"] + test_df[\"insult\"] + test_df[\"identity_hate\"])\n",
    "test_df[\"non_toxic\"] = test_df[\"total_score\"].map(non_toxic_mapper)\n",
    "test_df = test_df.drop(\"total_score\", axis=1)\n",
    "\n",
    "y = test_df[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate', 'non_toxic']]\n",
    "y = y.to_dict(\"index\")\n",
    "\n",
    "test_data = list(zip(test_df['comment_text'],[{'cats': cats} for cats in y.values()]))\n",
    "print(test_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a6182303",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-23T01:28:54.617302Z",
     "start_time": "2022-12-23T01:28:54.610951Z"
    }
   },
   "outputs": [],
   "source": [
    "del test_df, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "a79d01b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-23T01:33:23.779809Z",
     "start_time": "2022-12-23T01:30:15.991631Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8009914e39f84630a4baca7dc225a06c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63978 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'token_acc': 1.0,\n",
       " 'token_p': 1.0,\n",
       " 'token_r': 1.0,\n",
       " 'token_f': 1.0,\n",
       " 'cats_score': 0.9641280264206736,\n",
       " 'cats_score_desc': 'macro AUC',\n",
       " 'cats_micro_p': 0.8869367009754647,\n",
       " 'cats_micro_r': 0.8672905735605609,\n",
       " 'cats_micro_f': 0.8770036257752019,\n",
       " 'cats_macro_p': 0.6219961574970904,\n",
       " 'cats_macro_r': 0.5404661894781261,\n",
       " 'cats_macro_f': 0.554518821023627,\n",
       " 'cats_macro_auc': 0.9641280264206736,\n",
       " 'cats_f_per_type': {'toxic': {'p': 0.5440420299575229,\n",
       "   'r': 0.7991789819376026,\n",
       "   'f': 0.6473796222399575},\n",
       "  'severe_toxic': {'p': 0.3881578947368421,\n",
       "   'r': 0.3215258855585831,\n",
       "   'f': 0.3517138599105812},\n",
       "  'obscene': {'p': 0.6039206950323012,\n",
       "   'r': 0.7344892982931455,\n",
       "   'f': 0.6628361858190708},\n",
       "  'threat': {'p': 0.46153846153846156,\n",
       "   'r': 0.2559241706161137,\n",
       "   'f': 0.32926829268292684},\n",
       "  'insult': {'p': 0.6931993817619784,\n",
       "   'r': 0.5234899328859061,\n",
       "   'f': 0.5965087281795511},\n",
       "  'identity_hate': {'p': 0.6818181818181818,\n",
       "   'r': 0.23174157303370788,\n",
       "   'f': 0.34591194968553457},\n",
       "  'non_toxic': {'p': 0.9812964576343448,\n",
       "   'r': 0.9169134840218238,\n",
       "   'f': 0.9480131086477678}},\n",
       " 'cats_auc_per_type': {'toxic': 0.9522815361252487,\n",
       "  'severe_toxic': 0.9807639134269659,\n",
       "  'obscene': 0.9710395929356551,\n",
       "  'threat': 0.9674838126987343,\n",
       "  'insult': 0.9619473564420757,\n",
       "  'identity_hate': 0.9612523629497998,\n",
       "  'non_toxic': 0.954127610366236},\n",
       " 'speed': 64376.93879080622}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = []\n",
    "\n",
    "for texts, annotation in tqdm(test_data):\n",
    "    doc = nlp_train.make_doc(texts)\n",
    "    test.append(Example.from_dict(doc, annotation))  \n",
    "    \n",
    "score = nlp_train.evaluate(test)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2040867e",
   "metadata": {},
   "source": [
    "## Saving the model to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7d25e9ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-23T00:44:31.709895Z",
     "start_time": "2022-12-23T00:44:31.224574Z"
    }
   },
   "outputs": [],
   "source": [
    "nlp_train.to_disk('main-model/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 734.8438120000001,
   "position": {
    "height": "40px",
    "left": "1307.77px",
    "right": "20px",
    "top": "121px",
    "width": "476.006px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
