{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b9ebeba",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Importing-packages-&amp;-dataset\" data-toc-modified-id=\"Importing-packages-&amp;-dataset-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Importing packages &amp; dataset</a></span></li><li><span><a href=\"#Exploring-the-dataset\" data-toc-modified-id=\"Exploring-the-dataset-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Exploring the dataset</a></span><ul class=\"toc-item\"><li><span><a href=\"#General-features\" data-toc-modified-id=\"General-features-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>General features</a></span></li><li><span><a href=\"#Getting-to-know-the-labels\" data-toc-modified-id=\"Getting-to-know-the-labels-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Getting to know the labels</a></span><ul class=\"toc-item\"><li><span><a href=\"#Can-a-comment-be-both-toxic-and-severely-toxic?\" data-toc-modified-id=\"Can-a-comment-be-both-toxic-and-severely-toxic?-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>Can a comment be both toxic and severely toxic?</a></span></li><li><span><a href=\"#Do-the-obsene/threat/insult/identity_hate-tags-come-under-the-toxic-umbrella?\" data-toc-modified-id=\"Do-the-obsene/threat/insult/identity_hate-tags-come-under-the-toxic-umbrella?-2.2.2\"><span class=\"toc-item-num\">2.2.2&nbsp;&nbsp;</span>Do the obsene/threat/insult/identity_hate tags come under the toxic umbrella?</a></span></li><li><span><a href=\"#Adding-a-non-toxic-column:\" data-toc-modified-id=\"Adding-a-non-toxic-column:-2.2.3\"><span class=\"toc-item-num\">2.2.3&nbsp;&nbsp;</span>Adding a non-toxic column:</a></span></li><li><span><a href=\"#Removing-the-total_score-column\" data-toc-modified-id=\"Removing-the-total_score-column-2.2.4\"><span class=\"toc-item-num\">2.2.4&nbsp;&nbsp;</span>Removing the total_score column</a></span></li></ul></li></ul></li><li><span><a href=\"#Cleaning-the-comment_text-column\" data-toc-modified-id=\"Cleaning-the-comment_text-column-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Cleaning the comment_text column</a></span><ul class=\"toc-item\"><li><span><a href=\"#IP-address\" data-toc-modified-id=\"IP-address-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>IP address</a></span></li><li><span><a href=\"#Date-Time\" data-toc-modified-id=\"Date-Time-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Date Time</a></span></li><li><span><a href=\"#Formatting-characters\" data-toc-modified-id=\"Formatting-characters-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Formatting characters</a></span></li><li><span><a href=\"#Writing-functions-for-future-use-on-unseen-comments-data\" data-toc-modified-id=\"Writing-functions-for-future-use-on-unseen-comments-data-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Writing functions for future use on unseen comments data</a></span></li></ul></li><li><span><a href=\"#Modeling-Trial-1:-Basic-spaCy-Multi-Label-Classification\" data-toc-modified-id=\"Modeling-Trial-1:-Basic-spaCy-Multi-Label-Classification-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Modeling Trial 1: Basic spaCy Multi-Label Classification</a></span><ul class=\"toc-item\"><li><span><a href=\"#Converting-the-data-into-a-spaCy-readable-format\" data-toc-modified-id=\"Converting-the-data-into-a-spaCy-readable-format-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Converting the data into a spaCy-readable format</a></span></li><li><span><a href=\"#Train-test-splitting-the-data\" data-toc-modified-id=\"Train-test-splitting-the-data-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Train-test splitting the data</a></span></li><li><span><a href=\"#Pre-processing-the-data-using-a-pre-made-spaCy-pipeline\" data-toc-modified-id=\"Pre-processing-the-data-using-a-pre-made-spaCy-pipeline-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Pre-processing the data using a pre-made spaCy pipeline</a></span><ul class=\"toc-item\"><li><span><a href=\"#Reloading-without-the-steps-we-won't-require\" data-toc-modified-id=\"Reloading-without-the-steps-we-won't-require-4.3.1\"><span class=\"toc-item-num\">4.3.1&nbsp;&nbsp;</span>Reloading without the steps we won't require</a></span></li><li><span><a href=\"#Adding-stop-word-removal\" data-toc-modified-id=\"Adding-stop-word-removal-4.3.2\"><span class=\"toc-item-num\">4.3.2&nbsp;&nbsp;</span>Adding stop word removal</a></span></li><li><span><a href=\"#Pre-processing:\" data-toc-modified-id=\"Pre-processing:-4.3.3\"><span class=\"toc-item-num\">4.3.3&nbsp;&nbsp;</span>Pre-processing:</a></span></li><li><span><a href=\"#Adding-the-labels-to-the-model\" data-toc-modified-id=\"Adding-the-labels-to-the-model-4.3.4\"><span class=\"toc-item-num\">4.3.4&nbsp;&nbsp;</span>Adding the labels to the model</a></span></li></ul></li></ul></li><li><span><a href=\"#Results-&amp;-predictions\" data-toc-modified-id=\"Results-&amp;-predictions-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Results &amp; predictions</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfe19cc",
   "metadata": {},
   "source": [
    "# Exploring spaCy: Toxicity Levels "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604e2a5a",
   "metadata": {},
   "source": [
    "**Overview**\n",
    "\n",
    "Using the SpaCy package to explore the [Toxic Comment Classification Challenge](https://www.kaggle.com/competitions/jigsaw-toxic-comment-classification-challenge/overview). \n",
    "\n",
    "- Challenge: build a model that’s capable of detecting different types of of toxicity like **threats, obscenity, insults, and identity-based hate**. \n",
    "- The dataset contains comments from Wikipedia’s talk page edits. \n",
    "- The goal is to help online discussion become more productive and respectful.\n",
    "\n",
    "**Dataset Description**\n",
    "\n",
    "A large number of Wikipedia comments which have been labeled by human raters for toxic behavior. \n",
    "The types of toxicity are:\n",
    "\n",
    "- toxic\n",
    "- severe_toxic\n",
    "- obscene\n",
    "- threat\n",
    "- insult\n",
    "- identity_hate\n",
    "\n",
    "You must create a model which predicts a **probability of each type of toxicity** for each comment.\n",
    "\n",
    "**File descriptions**\n",
    "\n",
    "- train.csv - the training set, contains comments with their binary labels\n",
    "- test.csv - the test set, you must predict the toxicity probabilities for these comments.\n",
    "- sample_submission.csv - a sample submission file in the correct format\n",
    "- test_labels.csv - labels for the test data; value of -1 indicates it was not used for scoring; (Note: file added after competition close!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bef354",
   "metadata": {},
   "source": [
    "## Importing packages & dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dffff7c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-22T14:32:18.761215Z",
     "start_time": "2022-12-22T14:32:15.949948Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "np.set_printoptions(precision=4)\n",
    "sns.set(font_scale=1.5)\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a82cb910",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-22T14:32:19.644149Z",
     "start_time": "2022-12-22T14:32:18.763472Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset contains 159571 entries with 8 features.\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"/Users/anastasiakuzmich/Desktop/jigsaw-toxic-comment-classification-challenge/train.csv\")\n",
    "\n",
    "print(\"The dataset contains %s entries with %s features.\" \n",
    "      % (df.shape[0], df.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d366e9",
   "metadata": {},
   "source": [
    "## Exploring the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ccc41c",
   "metadata": {},
   "source": [
    "### General features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "257b9473",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-22T14:32:19.717883Z",
     "start_time": "2022-12-22T14:32:19.646223Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159571 entries, 0 to 159570\n",
      "Data columns (total 8 columns):\n",
      " #   Column         Non-Null Count   Dtype \n",
      "---  ------         --------------   ----- \n",
      " 0   id             159571 non-null  object\n",
      " 1   comment_text   159571 non-null  object\n",
      " 2   toxic          159571 non-null  int64 \n",
      " 3   severe_toxic   159571 non-null  int64 \n",
      " 4   obscene        159571 non-null  int64 \n",
      " 5   threat         159571 non-null  int64 \n",
      " 6   insult         159571 non-null  int64 \n",
      " 7   identity_hate  159571 non-null  int64 \n",
      "dtypes: int64(6), object(2)\n",
      "memory usage: 9.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9129a454",
   "metadata": {},
   "source": [
    "✏️ Okay, there's no missing values & the data types are fine. Within this dataset there is:\n",
    "\n",
    "1. An id column which might just be easier to drop. \n",
    "2. The comment_text column which contains the content of the comment itself\n",
    "3. The columns indicating the comment's annotation as toxic, severe_toxic, obscene, threat, insult or identity hate. Let's explore these further:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aba66928",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-22T14:32:19.764940Z",
     "start_time": "2022-12-22T14:32:19.721929Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.095844</td>\n",
       "      <td>0.009996</td>\n",
       "      <td>0.052948</td>\n",
       "      <td>0.002996</td>\n",
       "      <td>0.049364</td>\n",
       "      <td>0.008805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.294379</td>\n",
       "      <td>0.099477</td>\n",
       "      <td>0.223931</td>\n",
       "      <td>0.054650</td>\n",
       "      <td>0.216627</td>\n",
       "      <td>0.093420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               toxic   severe_toxic        obscene         threat  \\\n",
       "count  159571.000000  159571.000000  159571.000000  159571.000000   \n",
       "mean        0.095844       0.009996       0.052948       0.002996   \n",
       "std         0.294379       0.099477       0.223931       0.054650   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.000000       0.000000       0.000000       0.000000   \n",
       "50%         0.000000       0.000000       0.000000       0.000000   \n",
       "75%         0.000000       0.000000       0.000000       0.000000   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "              insult  identity_hate  \n",
       "count  159571.000000  159571.000000  \n",
       "mean        0.049364       0.008805  \n",
       "std         0.216627       0.093420  \n",
       "min         0.000000       0.000000  \n",
       "25%         0.000000       0.000000  \n",
       "50%         0.000000       0.000000  \n",
       "75%         0.000000       0.000000  \n",
       "max         1.000000       1.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb210a8",
   "metadata": {},
   "source": [
    "✏️ The labels are binary, so a comment is either of that label, or not of that label. 9% of the comments are toxic, 5% are obscene, 4% are insults, less than 1% are severely toxic, identity theft or a threat. \n",
    "\n",
    "Let's find out if these labels are are mutually exclusive + how many non-toxic comments there are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1262ee9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-22T14:32:19.778815Z",
     "start_time": "2022-12-22T14:32:19.769137Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.898321\n",
       "1    0.039857\n",
       "3    0.026377\n",
       "2    0.021808\n",
       "4    0.011030\n",
       "5    0.002413\n",
       "6    0.000194\n",
       "Name: total_score, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['total_score'] = (df['toxic'] \n",
    "                     + df[\"severe_toxic\"] \n",
    "                     + df[\"obscene\"] \n",
    "                     + df[\"threat\"]\n",
    "                     + df[\"insult\"]\n",
    "                     + df[\"identity_hate\"])\n",
    "\n",
    "df['total_score'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66a9906",
   "metadata": {},
   "source": [
    "✏️ My takeaways at this stage:\n",
    "\n",
    "- 90% of the comments aren't negative. That's good to know, but we are dealing with severe class imbalance...\n",
    "- A good amount of comments ticks several boxes, so the labels are not exclusive. Some tick all 6 somehow? (*How angry do you have to be?*)\n",
    "- The model needs to be capable of detecting different types of of toxicity like threats, obscenity, insults, and identity-based hate. I need to think about how I'll need to restructure the data and what I want my predictors to be.\n",
    "\n",
    "**Next questions to address:**\n",
    "\n",
    "1. Can a comment be both toxic and severely toxic? \n",
    "2. Are toxic & severe toxic the umbrella categories, then obsene/threat/insult/identity_hate their sub-categories? Restructure and see.\n",
    "3. Perhaps at this stage it is worth introducing a non-toxic column? So the model would almost predict whether the comments are toxic vs non-toxic vs severely toxic, then *if* it's toxic then what subcategory of toxic?\n",
    "\n",
    "### Getting to know the labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f9c86d",
   "metadata": {},
   "source": [
    "#### Can a comment be both toxic and severely toxic?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52acc279",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-22T14:32:19.802151Z",
     "start_time": "2022-12-22T14:32:19.780245Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of toxic comments: 15294\n",
      "Number of severely toxic comments: 1595\n",
      "Number of severely toxic and toxic comments: 1595\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of toxic comments:\", len(df[df['toxic'] == 1]))\n",
    "print(\"Number of severely toxic comments:\", len(df[df['severe_toxic'] == 1]))\n",
    "print(\"Number of severely toxic and toxic comments:\", len(df[(df['toxic'] == 1) & (df['severe_toxic'] == 1)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e4b26a",
   "metadata": {},
   "source": [
    "✏️ Yes. All severely toxic comments are also toxic, but not all toxic comments are severe. Perhaps it's worth making them exclusive within the dataframe? Could also do it later on after modelling to see if it would improve the score. \n",
    "\n",
    "#### Do the obsene/threat/insult/identity_hate tags come under the toxic umbrella?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12c69bcf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-22T14:32:19.815409Z",
     "start_time": "2022-12-22T14:32:19.803680Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of obsene comments that are not toxic: 523\n",
      "Number of threatening comments that are not toxic: 29\n",
      "Number of insulting comments that are not toxic: 533\n",
      "Number of identity-hateful comments that are not toxic: 103\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of obsene comments that are not toxic:\",\n",
    "      len(df[(df['toxic'] == 0) & (df['obscene'] == 1)]))\n",
    "\n",
    "print(\"Number of threatening comments that are not toxic:\",\n",
    "      len(df[(df['toxic'] == 0) & (df['threat'] == 1)]))\n",
    "\n",
    "print(\"Number of insulting comments that are not toxic:\",\n",
    "      len(df[(df['toxic'] == 0) & (df['insult'] == 1)]))\n",
    "\n",
    "print(\"Number of identity-hateful comments that are not toxic:\",\n",
    "      len(df[(df['toxic'] == 0) & (df['identity_hate'] == 1)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ff83f9",
   "metadata": {},
   "source": [
    "✏️ No, apparently they can be mutually exclusive? Which doesn't make complete sense to me. Let's inspect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b29fdb9b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-22T14:32:19.830966Z",
     "start_time": "2022-12-22T14:32:19.818618Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comment  1 :  obscene \n",
      " How do you know he is dead.  Its just his plane that crashed.  Jeezz, quit busting his nuts, folks. \n",
      "\n",
      "Comment  1 :  threat \n",
      " Please stop. If you continue to ignore our policies by introducing inappropriate pages to Wikipedia, you will be blocked. \n",
      "\n",
      "Comment  1 :  insult \n",
      " REPLY ABOVE:\n",
      "That was me, loser. The UN defines Vietnam to be part of Southeast Asia. And far as I know Vietnam is part of ASEAN, and used to be part of French Indochina with Laos and all those shit countries Anyway your culture has always been more influenced by SEA than Han Chinese (as in proper Yangtze Han Chinese, not the fringe indigenous tribes in Guangzhou/Guangxi). \n",
      "\n",
      "Just admit that you vietnamese are all a bunch of wannabe crap people. ALL the east asian people I've spoken to thinks of Vietnam as a very integral part of SEA, and we all think you're backward, dirty and speak in a horrible swearing language. Doesn't matter what crap you spout on Wikipedia, won't change the way people in the real world think. \n",
      "\n",
      "Comment  1 :  identity_hate \n",
      " Mate, sound like you are jewish\n",
      "\n",
      "Gayness is in the air \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for category in [\"obscene\", \"threat\", \"insult\", \"identity_hate\"]:\n",
    "    for i in range(1):\n",
    "        print(\"Comment \", i+1, \": \", str(category),\n",
    "              \"\\n\", \n",
    "              df[(df['toxic'] == 0) & (df[category] == 1)][\"comment_text\"].iloc[i], \n",
    "              \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9185ae09",
   "metadata": {},
   "source": [
    "✏️ Ummm... these look toxic to me? But I just re-read the challenge rules and I'm expected to classify each of the columns so I'll try not to overthink why these categories are the way they are. I will make an extra column for non-toxic, neutral comments though because in my head that will make the probabilities make sense?\n",
    "\n",
    "#### Adding a non-toxic column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e96b6a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-22T14:32:19.886418Z",
     "start_time": "2022-12-22T14:32:19.832635Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    143346\n",
       "0     16225\n",
       "Name: non_toxic, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def non_toxic_mapper(x):\n",
    "    if x == 0: \n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "df[\"non_toxic\"] = df[\"total_score\"].map(non_toxic_mapper)\n",
    "df[\"non_toxic\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d19c09",
   "metadata": {},
   "source": [
    "#### Removing the total_score column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fbc640",
   "metadata": {},
   "source": [
    "This column was only used to explore the data, so I'm dropping it before I start the pre-modelling stages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85407695",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-22T14:32:19.924198Z",
     "start_time": "2022-12-22T14:32:19.889915Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>non_toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  non_toxic  \n",
       "0             0        0       0       0              0          1  \n",
       "1             0        0       0       0              0          1  \n",
       "2             0        0       0       0              0          1  \n",
       "3             0        0       0       0              0          1  \n",
       "4             0        0       0       0              0          1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop(\"total_score\", axis=1)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ed98dd",
   "metadata": {},
   "source": [
    "## Cleaning the comment_text column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9dae9a",
   "metadata": {},
   "source": [
    "✏️ Having read up on spaCy, it appears that its pipelines work best on natural sentences because that's what its training data looks like. Extensive preprocessing, like removing stop words and lowercasing will ззrently make things worse with spaCy because it uses that information for clues - [StackOverFlow](https://stackoverflow.com/a/70502883) .\n",
    "\n",
    "That said, I will still be removing newlines and formatting characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17d1aec8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-22T14:32:19.930707Z",
     "start_time": "2022-12-22T14:32:19.925962Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Explanation\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspecting examples\n",
    "\n",
    "df[\"comment_text\"].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455f9caf",
   "metadata": {},
   "source": [
    "✏️ Printing these would make the formatting characters invisible, so I ran the below in individual cells..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1caa1b78",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-22T14:32:19.940412Z",
     "start_time": "2022-12-22T14:32:19.937043Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df[\"comment_text\"].iloc[1]\n",
    "# df[\"comment_text\"].iloc[5]\n",
    "# df[\"comment_text\"].iloc[10]\n",
    "# df[\"comment_text\"].iloc[20]\n",
    "# df[\"comment_text\"].iloc[21]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999500e1",
   "metadata": {},
   "source": [
    "✏️ \n",
    "\n",
    "**To remove:**\n",
    "\n",
    "Having inspected some of the comments, I decided I will be removing the following features from the data:\n",
    "\n",
    "- IP addresses\n",
    "- Date & Time (Format: + 21:51, January 11, 2016 (UTC))\n",
    "- Newline characters (\\n) & \"\\\\\" separator\n",
    "- Non-breaking spaces (\\xa0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0fa74e",
   "metadata": {},
   "source": [
    "### IP address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91e43b0f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-22T14:32:19.950205Z",
     "start_time": "2022-12-22T14:32:19.943514Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['89.205.38.27']\n",
      "['93.161.107.169']\n",
      "['70.100.229.154']\n",
      "['76.122.79.82']\n",
      "['201.215.187.159']\n",
      "['109.77.58.139', '109.76.191.188', '109.76.191.188', '109.78.224.50']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "IpAddressRegex = \"(\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3})\"\n",
    "\n",
    "for i in range(100):\n",
    "    txt = df[\"comment_text\"].iloc[i]\n",
    "    x = re.findall(IpAddressRegex, txt)\n",
    "    if len(x) > 0:\n",
    "        print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bc1307",
   "metadata": {},
   "source": [
    "✏️ The regex works, so I'll use it to clean now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a85def46",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-22T14:32:21.566650Z",
     "start_time": "2022-12-22T14:32:19.953074Z"
    }
   },
   "outputs": [],
   "source": [
    "df[\"comment_text\"] = df[\"comment_text\"].replace(IpAddressRegex,'', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad4246d",
   "metadata": {},
   "source": [
    "### Date Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8b9f8a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-22T14:32:21.585328Z",
     "start_time": "2022-12-22T14:32:21.569176Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('21:51, January 11, 2016 (UTC)', '')]\n",
      "[('', '18:01, 16 Jun 2005 (UTC)')]\n",
      "[('28:57, August 19, 2007 (UTC)', '')]\n"
     ]
    }
   ],
   "source": [
    "DateTimeRegex = \"(\\d{2}\\:\\d{2}\\, [A-Z][a-z]{2,8}\\ \\d{1,2}\\, \\d{4}\\ \\([A-Z]{3}\\))|(\\d{2}\\:\\d{2}\\, \\d{2}\\ [A-Z][a-z]{2}\\ \\d{4}\\ \\([A-Z]{3}\\))\"\n",
    "\n",
    "for i in range(50):\n",
    "    txt = df[\"comment_text\"].iloc[i]\n",
    "    x = re.findall(DateTimeRegex, txt)\n",
    "    if len(x) > 0:\n",
    "        print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b78a46d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-22T14:32:24.789501Z",
     "start_time": "2022-12-22T14:32:21.588291Z"
    }
   },
   "outputs": [],
   "source": [
    "df[\"comment_text\"] = df[\"comment_text\"].replace(DateTimeRegex,'', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89063ca3",
   "metadata": {},
   "source": [
    "**Checking for other pattern types:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e902a5c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-22T14:32:24.921275Z",
     "start_time": "2022-12-22T14:32:24.792045Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "459       \", 20 January 2013 (UTC)\\nThe argument at WP:N...\n",
       "594       \"\\n\\n\"\"For Security Reasons\"\" edit war\\nAs I s...\n",
       "601       All the sources needed are provided, all you n...\n",
       "637       \"40, 25 January 2008 (UTC)\\n\\nNO, i am NOT bei...\n",
       "709       \"\\nAmerican Psychologist as additional resourc...\n",
       "                                ...                        \n",
       "159090    January 2008 (UTC)\\n\\nUnlike other schools, th...\n",
       "159383    I know the images I've uploaded in the past we...\n",
       "159395    \"\\n\\n feedback as requested \\n\\nHey Antonin! I...\n",
       "159427                         for January 2006 discussion.\n",
       "159460    , 24 January 2010 (UTC)\\nWikipedia is hosted i...\n",
       "Name: comment_text, Length: 1408, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['comment_text'].str.contains('Jan')][\"comment_text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31be7846",
   "metadata": {},
   "source": [
    "**Patterns to add:**\n",
    "\n",
    "- 10:36, 5 January 2012\n",
    "- 27 January 2010 (UTC)\n",
    "- \"Semi-protected edit request on \"7 January 2014\n",
    "- 01:29, 30\n",
    "- January 2013 (UTC)\n",
    "\n",
    "✏️ The rest of the dates are mostly mentioned in context. The data should be clear after I add the below formats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cf2b929e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-22T14:32:35.930787Z",
     "start_time": "2022-12-22T14:32:24.923314Z"
    }
   },
   "outputs": [],
   "source": [
    "updated_DateTimeRegex = \"(\\d{2}\\:\\d{2}\\, [A-Z][a-z]{2,8}\\ \\d{1,2}\\, \\d{4}\\ \\([A-Z]{3}\\))|(\\d{2}\\:\\d{2}\\, \\d{2}\\ [A-Z][a-z]{2}\\ \\d{4}\\ \\([A-Z]{3}\\))|(\\d{2}\\:\\d{2}\\, \\d{1,2}\\ [A-Z][a-z]{2,8} \\d{4})|(\\d{1,2}\\ [A-Z][a-z]{2,8}\\ \\d{4}\\ \\([A-Z]{3}\\))|(\\d{1,2}\\ [A-Z][a-z]{1,7}\\ \\d{4})|(\\d{2}\\:\\d{2}\\, \\d{1,2})|([A-Z][a-z]{2,8}\\ \\d{4}\\ \\([A-Z]{3}\\))\"\n",
    "df['comment_text'] = df['comment_text'].replace(updated_DateTimeRegex,'', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d379c828",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-21T15:45:19.065168Z",
     "start_time": "2022-12-21T15:45:19.062135Z"
    }
   },
   "source": [
    "### Formatting characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1c058e99",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-22T14:32:36.492053Z",
     "start_time": "2022-12-22T14:32:35.933171Z"
    }
   },
   "outputs": [],
   "source": [
    "df['comment_text'] = df[\"comment_text\"].replace('\\\\n', ' ', regex=True)\n",
    "df['comment_text'] = df['comment_text'].replace(r'\\\\', ' ', regex=True)\n",
    "df['comment_text'] = df['comment_text'].replace(u'\\xa0', u' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f24e5b",
   "metadata": {},
   "source": [
    "### Writing functions for future use on unseen comments data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fffeec13",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-22T14:32:36.502012Z",
     "start_time": "2022-12-22T14:32:36.494970Z"
    }
   },
   "outputs": [],
   "source": [
    "def non_toxic_mapper(x):\n",
    "    \n",
    "    \"\"\"Maps the non-toxic column based on the total score column.\"\"\"\n",
    "    \n",
    "    if x == 0: \n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def add_non_toxic_column(df):\n",
    "    \n",
    "    \"\"\"Adds a non-toxic column.\"\"\"\n",
    "    \n",
    "    df['total_score'] = (df['toxic'] + df[\"severe_toxic\"] \n",
    "                         + df[\"obscene\"] + df[\"threat\"]\n",
    "                         + df[\"insult\"] + df[\"identity_hate\"])\n",
    "\n",
    "    df[\"non_toxic\"] = df[\"total_score\"].map(non_toxic_mapper)\n",
    "    df = df.drop(\"total_score\", axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def clean_comments_column(df):\n",
    "    \n",
    "    \"\"\"Cleans the comments column.\"\"\"\n",
    "    \n",
    "    import re\n",
    "    \n",
    "    IpAddressRegex = \"(\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3})\"\n",
    "    DateTimeRegex = \"(\\d{2}\\:\\d{2}\\, [A-Z][a-z]{2,8}\\ \\d{1,2}\\, \\d{4}\\ \\([A-Z]{3}\\))|(\\d{2}\\:\\d{2}\\, \\d{2}\\ [A-Z][a-z]{2}\\ \\d{4}\\ \\([A-Z]{3}\\))|(\\d{2}\\:\\d{2}\\, \\d{1,2}\\ [A-Z][a-z]{2,8} \\d{4})|(\\d{1,2}\\ [A-Z][a-z]{2,8}\\ \\d{4}\\ \\([A-Z]{3}\\))|(\\d{1,2}\\ [A-Z][a-z]{1,7}\\ \\d{4})|(\\d{2}\\:\\d{2}\\, \\d{1,2})|([A-Z][a-z]{2,8}\\ \\d{4}\\ \\([A-Z]{3}\\))\"\n",
    "    \n",
    "    features_to_remove = [IpAddressRegex,\n",
    "                          DateTimeRegex,\n",
    "                          '\\\\n', r'\\\\']\n",
    "    \n",
    "    for feature in features_to_remove:\n",
    "        df[\"comment_text\"] = df[\"comment_text\"].replace(feature,' ', regex=True)\n",
    " \n",
    "    df['comment_text'] = df['comment_text'].replace(u'\\xa0', u' ')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ee63e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-21T16:01:14.152954Z",
     "start_time": "2022-12-21T16:01:14.098755Z"
    }
   },
   "source": [
    "## Modeling Trial 1: Basic spaCy Multi-Label Classification "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e13cf5b",
   "metadata": {},
   "source": [
    "### Converting the data into a spaCy-readable format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c6edf0b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-22T14:32:36.508306Z",
     "start_time": "2022-12-22T14:32:36.504378Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'comment_text', 'toxic', 'severe_toxic', 'obscene', 'threat',\n",
       "       'insult', 'identity_hate', 'non_toxic'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "58d189af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-22T14:32:36.987736Z",
     "start_time": "2022-12-22T14:32:36.509910Z"
    }
   },
   "outputs": [],
   "source": [
    "# Extracting the labels\n",
    "y = df[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate', 'non_toxic']]\n",
    "\n",
    "# Labels variable to be used in the pipeline\n",
    "labels = list(y.columns)\n",
    "\n",
    "# Converting the labels to a spacy-compatible dictionary \n",
    "y = y.to_dict(\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "59f5d5cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-22T14:32:37.014305Z",
     "start_time": "2022-12-22T14:32:36.990049Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X = list(df[\"comment_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2a6f9a",
   "metadata": {},
   "source": [
    "### Train-test splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "56456fb8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-22T14:32:41.717832Z",
     "start_time": "2022-12-22T14:32:37.016143Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(np.array(X), y, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=13)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61a6a7c",
   "metadata": {},
   "source": [
    "✏️ I'm optimising performance by converting the comment text into an array, but have to leave the dictionary as is for compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4ef35fec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-22T14:32:41.739538Z",
     "start_time": "2022-12-22T14:32:41.720329Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(127656,) (31915,)\n",
      "127656 31915\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_test.shape)\n",
    "print(len(y_train),  len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3987dd03",
   "metadata": {},
   "source": [
    "### Pre-processing the data using a pre-made spaCy pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b0c7f96d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-22T14:32:48.942602Z",
     "start_time": "2022-12-22T14:32:41.742106Z"
    }
   },
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "22ed5c32",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-22T14:32:50.387971Z",
     "start_time": "2022-12-22T14:32:48.945257Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'ner', 'attribute_ruler', 'lemmatizer']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_preprocess = spacy.load(\"en_core_web_md\")\n",
    "nlp_preprocess.pipe_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4939ff2c",
   "metadata": {},
   "source": [
    "✏️ I want to use the following pre-processing steps in my pipeline:\n",
    "\n",
    "- Token-to-vector embedding (tok2vec) \n",
    "- Lemmatization (lemmatizer)\n",
    "- Part of speech tagging (tagger)\n",
    "- Dependency parsing (parser)\n",
    "- Stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b31fc7",
   "metadata": {},
   "source": [
    "#### Reloading without the steps we won't require "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f2f1bb90",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-22T11:24:23.328559Z",
     "start_time": "2022-12-22T11:24:22.273830Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'lemmatizer']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\", exclude=[\"ner\", \"attribute_ruler\"])\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1c367c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-22T10:38:40.305103Z",
     "start_time": "2022-12-22T10:38:40.301377Z"
    }
   },
   "source": [
    "#### Adding stop word removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a8b2fd28",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-22T11:24:42.842225Z",
     "start_time": "2022-12-22T11:24:42.837657Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stop words: 326\n",
      "First ten stop words: ['onto', 'we', 'various', 'she', 'n’t', \"'s\", 'what', 'call', 'latterly', 'nowhere']\n"
     ]
    }
   ],
   "source": [
    "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "print('Number of stop words: %d' % len(spacy_stopwords))\n",
    "print('First ten stop words: %s' % list(spacy_stopwords)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "069f8d28",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-22T11:25:05.822541Z",
     "start_time": "2022-12-22T11:25:05.809547Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tok2vec', 'stop_word_remover', 'tagger', 'parser', 'lemmatizer']\n"
     ]
    }
   ],
   "source": [
    "from spacy.language import Language\n",
    "\n",
    "@Language.component(\"stop_word_remover\")\n",
    "\n",
    "def stop_word_remover(doc):\n",
    "    tokens = [token.text for token in doc if not token.is_stop]\n",
    "    return tokens\n",
    "\n",
    "nlp.add_pipe(\"stop_word_remover\", before=\"tagger\")\n",
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50507974",
   "metadata": {},
   "source": [
    "#### Pre-processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3fcc92c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-22T11:37:55.364228Z",
     "start_time": "2022-12-22T11:37:54.602390Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'tensor'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [72]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokens\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DocBin\n\u001b[1;32m      3\u001b[0m db \u001b[38;5;241m=\u001b[39m DocBin()\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m nlp\u001b[38;5;241m.\u001b[39mpipe(train_data, as_tuples\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m      6\u001b[0m     db\u001b[38;5;241m.\u001b[39madd(doc)\n\u001b[1;32m      8\u001b[0m db\u001b[38;5;241m.\u001b[39mto_disk(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./preprocessed_train_data.spacy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/spacy/language.py:1452\u001b[0m, in \u001b[0;36mLanguage.pipe\u001b[0;34m(self, texts, as_tuples, batch_size, disable, component_cfg, n_process)\u001b[0m\n\u001b[1;32m   1444\u001b[0m contexts \u001b[38;5;241m=\u001b[39m (tc[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m tc \u001b[38;5;129;01min\u001b[39;00m text_context2)\n\u001b[1;32m   1445\u001b[0m docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpipe(\n\u001b[1;32m   1446\u001b[0m     texts,\n\u001b[1;32m   1447\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1450\u001b[0m     component_cfg\u001b[38;5;241m=\u001b[39mcomponent_cfg,\n\u001b[1;32m   1451\u001b[0m )\n\u001b[0;32m-> 1452\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc, context \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(docs, contexts):\n\u001b[1;32m   1453\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m (doc, context)\n\u001b[1;32m   1454\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/spacy/language.py:1485\u001b[0m, in \u001b[0;36mLanguage.pipe\u001b[0;34m(self, texts, as_tuples, batch_size, disable, component_cfg, n_process)\u001b[0m\n\u001b[1;32m   1483\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m pipe \u001b[38;5;129;01min\u001b[39;00m pipes:\n\u001b[1;32m   1484\u001b[0m         docs \u001b[38;5;241m=\u001b[39m pipe(docs)\n\u001b[0;32m-> 1485\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m docs:\n\u001b[1;32m   1486\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m doc\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/spacy/util.py:1467\u001b[0m, in \u001b[0;36m_pipe\u001b[0;34m(docs, proc, name, default_error_handler, kwargs)\u001b[0m\n\u001b[1;32m   1465\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_pipe\u001b[39m(docs, proc, name, default_error_handler, kwargs):\n\u001b[1;32m   1466\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(proc, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpipe\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1467\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m proc\u001b[38;5;241m.\u001b[39mpipe(docs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1468\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1469\u001b[0m         \u001b[38;5;66;03m# We added some args for pipe that __call__ doesn't expect.\u001b[39;00m\n\u001b[1;32m   1470\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(kwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/spacy/pipeline/pipe.pyx:53\u001b[0m, in \u001b[0;36mpipe\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/spacy/util.py:1467\u001b[0m, in \u001b[0;36m_pipe\u001b[0;34m(docs, proc, name, default_error_handler, kwargs)\u001b[0m\n\u001b[1;32m   1465\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_pipe\u001b[39m(docs, proc, name, default_error_handler, kwargs):\n\u001b[1;32m   1466\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(proc, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpipe\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1467\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m proc\u001b[38;5;241m.\u001b[39mpipe(docs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1468\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1469\u001b[0m         \u001b[38;5;66;03m# We added some args for pipe that __call__ doesn't expect.\u001b[39;00m\n\u001b[1;32m   1470\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(kwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/spacy/pipeline/transition_parser.pyx:189\u001b[0m, in \u001b[0;36mpipe\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/spacy/util.py:1422\u001b[0m, in \u001b[0;36mminibatch\u001b[0;34m(items, size)\u001b[0m\n\u001b[1;32m   1420\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m   1421\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(size_)\n\u001b[0;32m-> 1422\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mitertools\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mislice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1423\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(batch) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1424\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/spacy/util.py:1467\u001b[0m, in \u001b[0;36m_pipe\u001b[0;34m(docs, proc, name, default_error_handler, kwargs)\u001b[0m\n\u001b[1;32m   1465\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_pipe\u001b[39m(docs, proc, name, default_error_handler, kwargs):\n\u001b[1;32m   1466\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(proc, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpipe\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1467\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m proc\u001b[38;5;241m.\u001b[39mpipe(docs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1468\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1469\u001b[0m         \u001b[38;5;66;03m# We added some args for pipe that __call__ doesn't expect.\u001b[39;00m\n\u001b[1;32m   1470\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(kwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/spacy/pipeline/trainable_pipe.pyx:79\u001b[0m, in \u001b[0;36mpipe\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/spacy/util.py:1486\u001b[0m, in \u001b[0;36mraise_error\u001b[0;34m(proc_name, proc, docs, e)\u001b[0m\n\u001b[1;32m   1485\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_error\u001b[39m(proc_name, proc, docs, e):\n\u001b[0;32m-> 1486\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/spacy/pipeline/trainable_pipe.pyx:75\u001b[0m, in \u001b[0;36mspacy.pipeline.trainable_pipe.TrainablePipe.pipe\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/spacy/pipeline/tagger.pyx:111\u001b[0m, in \u001b[0;36mspacy.pipeline.tagger.Tagger.predict\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/thinc/model.py:315\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m OutT:\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;124;03m\"\"\"Call the model's `forward` function with `is_train=False`, and return\u001b[39;00m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;124;03m    only the output, instead of the `(output, callback)` tuple.\u001b[39;00m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/thinc/layers/chain.py:54\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     52\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 54\u001b[0m     Y, inc_layer_grad \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(inc_layer_grad)\n\u001b[1;32m     56\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/thinc/model.py:291\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 291\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/spacy/pipeline/tok2vec.py:302\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, inputs, is_train)\u001b[0m\n\u001b[1;32m    300\u001b[0m width \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mget_dim(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnO\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m inputs:\n\u001b[0;32m--> 302\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mdoc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    303\u001b[0m         \u001b[38;5;66;03m# But we do need to do *something* if the tensor hasn't been set.\u001b[39;00m\n\u001b[1;32m    304\u001b[0m         \u001b[38;5;66;03m# The compromise is to at least return data of the right shape,\u001b[39;00m\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;66;03m# so the output is valid.\u001b[39;00m\n\u001b[1;32m    306\u001b[0m         outputs\u001b[38;5;241m.\u001b[39mappend(model\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39malloc2f(\u001b[38;5;28mlen\u001b[39m(doc), width))\n\u001b[1;32m    307\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'tensor'"
     ]
    }
   ],
   "source": [
    "docs = nlp.pipe(train_data, as_tuples=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b1387c",
   "metadata": {},
   "source": [
    "✏️ The preprocessed data and the corresponding labels are now saved within the \"docs\" instance. We will now use it as input to train a new pipeline containing the text categoriser model. This is necessary, as I wanted to use the pre-trained pipeline above for pre-processing only, while tranining my own model for the categorisation itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "649d7c4e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-22T11:33:52.743875Z",
     "start_time": "2022-12-22T11:33:51.976261Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'tensor'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [69]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokens\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DocBin\n\u001b[0;32m----> 3\u001b[0m doc_bin \u001b[38;5;241m=\u001b[39m \u001b[43mDocBin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdocs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m doc_bin\u001b[38;5;241m.\u001b[39mto_disk(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./preprocessed_train_data.spacy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/spacy/tokens/_serialize.py:79\u001b[0m, in \u001b[0;36mDocBin.__init__\u001b[0;34m(self, attrs, store_user_data, docs)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstore_user_data \u001b[38;5;241m=\u001b[39m store_user_data\n\u001b[0;32m---> 79\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m docs:\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd(doc)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/spacy/language.py:1452\u001b[0m, in \u001b[0;36mLanguage.pipe\u001b[0;34m(self, texts, as_tuples, batch_size, disable, component_cfg, n_process)\u001b[0m\n\u001b[1;32m   1444\u001b[0m contexts \u001b[38;5;241m=\u001b[39m (tc[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m tc \u001b[38;5;129;01min\u001b[39;00m text_context2)\n\u001b[1;32m   1445\u001b[0m docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpipe(\n\u001b[1;32m   1446\u001b[0m     texts,\n\u001b[1;32m   1447\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1450\u001b[0m     component_cfg\u001b[38;5;241m=\u001b[39mcomponent_cfg,\n\u001b[1;32m   1451\u001b[0m )\n\u001b[0;32m-> 1452\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc, context \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(docs, contexts):\n\u001b[1;32m   1453\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m (doc, context)\n\u001b[1;32m   1454\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/spacy/language.py:1485\u001b[0m, in \u001b[0;36mLanguage.pipe\u001b[0;34m(self, texts, as_tuples, batch_size, disable, component_cfg, n_process)\u001b[0m\n\u001b[1;32m   1483\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m pipe \u001b[38;5;129;01min\u001b[39;00m pipes:\n\u001b[1;32m   1484\u001b[0m         docs \u001b[38;5;241m=\u001b[39m pipe(docs)\n\u001b[0;32m-> 1485\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m docs:\n\u001b[1;32m   1486\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m doc\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/spacy/util.py:1467\u001b[0m, in \u001b[0;36m_pipe\u001b[0;34m(docs, proc, name, default_error_handler, kwargs)\u001b[0m\n\u001b[1;32m   1465\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_pipe\u001b[39m(docs, proc, name, default_error_handler, kwargs):\n\u001b[1;32m   1466\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(proc, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpipe\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1467\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m proc\u001b[38;5;241m.\u001b[39mpipe(docs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1468\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1469\u001b[0m         \u001b[38;5;66;03m# We added some args for pipe that __call__ doesn't expect.\u001b[39;00m\n\u001b[1;32m   1470\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(kwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/spacy/pipeline/pipe.pyx:53\u001b[0m, in \u001b[0;36mpipe\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/spacy/util.py:1467\u001b[0m, in \u001b[0;36m_pipe\u001b[0;34m(docs, proc, name, default_error_handler, kwargs)\u001b[0m\n\u001b[1;32m   1465\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_pipe\u001b[39m(docs, proc, name, default_error_handler, kwargs):\n\u001b[1;32m   1466\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(proc, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpipe\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1467\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m proc\u001b[38;5;241m.\u001b[39mpipe(docs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1468\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1469\u001b[0m         \u001b[38;5;66;03m# We added some args for pipe that __call__ doesn't expect.\u001b[39;00m\n\u001b[1;32m   1470\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(kwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/spacy/pipeline/transition_parser.pyx:189\u001b[0m, in \u001b[0;36mpipe\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/spacy/util.py:1422\u001b[0m, in \u001b[0;36mminibatch\u001b[0;34m(items, size)\u001b[0m\n\u001b[1;32m   1420\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m   1421\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(size_)\n\u001b[0;32m-> 1422\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mitertools\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mislice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1423\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(batch) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1424\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/spacy/util.py:1467\u001b[0m, in \u001b[0;36m_pipe\u001b[0;34m(docs, proc, name, default_error_handler, kwargs)\u001b[0m\n\u001b[1;32m   1465\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_pipe\u001b[39m(docs, proc, name, default_error_handler, kwargs):\n\u001b[1;32m   1466\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(proc, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpipe\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1467\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m proc\u001b[38;5;241m.\u001b[39mpipe(docs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1468\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1469\u001b[0m         \u001b[38;5;66;03m# We added some args for pipe that __call__ doesn't expect.\u001b[39;00m\n\u001b[1;32m   1470\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(kwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/spacy/pipeline/trainable_pipe.pyx:79\u001b[0m, in \u001b[0;36mpipe\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/spacy/util.py:1486\u001b[0m, in \u001b[0;36mraise_error\u001b[0;34m(proc_name, proc, docs, e)\u001b[0m\n\u001b[1;32m   1485\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_error\u001b[39m(proc_name, proc, docs, e):\n\u001b[0;32m-> 1486\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/spacy/pipeline/trainable_pipe.pyx:75\u001b[0m, in \u001b[0;36mspacy.pipeline.trainable_pipe.TrainablePipe.pipe\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/spacy/pipeline/tagger.pyx:111\u001b[0m, in \u001b[0;36mspacy.pipeline.tagger.Tagger.predict\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/thinc/model.py:315\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m OutT:\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;124;03m\"\"\"Call the model's `forward` function with `is_train=False`, and return\u001b[39;00m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;124;03m    only the output, instead of the `(output, callback)` tuple.\u001b[39;00m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/thinc/layers/chain.py:54\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     52\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 54\u001b[0m     Y, inc_layer_grad \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(inc_layer_grad)\n\u001b[1;32m     56\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/thinc/model.py:291\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 291\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/spacy/pipeline/tok2vec.py:302\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, inputs, is_train)\u001b[0m\n\u001b[1;32m    300\u001b[0m width \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mget_dim(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnO\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m inputs:\n\u001b[0;32m--> 302\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mdoc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    303\u001b[0m         \u001b[38;5;66;03m# But we do need to do *something* if the tensor hasn't been set.\u001b[39;00m\n\u001b[1;32m    304\u001b[0m         \u001b[38;5;66;03m# The compromise is to at least return data of the right shape,\u001b[39;00m\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;66;03m# so the output is valid.\u001b[39;00m\n\u001b[1;32m    306\u001b[0m         outputs\u001b[38;5;241m.\u001b[39mappend(model\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39malloc2f(\u001b[38;5;28mlen\u001b[39m(doc), width))\n\u001b[1;32m    307\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'tensor'"
     ]
    }
   ],
   "source": [
    "\n",
    "doc_bin = DocBin(docs=docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0af1555",
   "metadata": {},
   "source": [
    "# Adding a classification component to the pipeline for tranining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b28e9f8b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-22T11:20:24.994578Z",
     "start_time": "2022-12-22T11:20:24.963491Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tok2vec', 'stop_word_remover', 'tagger', 'parser', 'lemmatizer', 'textcat_multilabel']\n"
     ]
    }
   ],
   "source": [
    "from spacy.pipeline.textcat_multilabel import DEFAULT_MULTI_TEXTCAT_MODEL\n",
    "\n",
    "config = {\n",
    "   \"threshold\": 0.5,\n",
    "   \"model\": DEFAULT_MULTI_TEXTCAT_MODEL,\n",
    "}\n",
    "\n",
    "textcat = nlp.add_pipe(\"textcat_multilabel\", config=config)\n",
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d59f80",
   "metadata": {},
   "source": [
    "#### Adding the labels to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "26cdb136",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-22T11:20:25.003975Z",
     "start_time": "2022-12-22T11:20:24.997154Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('toxic',\n",
       " 'severe_toxic',\n",
       " 'obscene',\n",
       " 'threat',\n",
       " 'insult',\n",
       " 'identity_hate',\n",
       " 'non_toxic')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for label in labels:\n",
    "    textcat.add_label(label)\n",
    "    \n",
    "textcat.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f7bf3556",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-22T11:20:25.873910Z",
     "start_time": "2022-12-22T11:20:25.006381Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E143] Labels for component 'tagger' not initialized. This can be fixed by calling add_label, or by providing a representative batch of examples to the component's `initialize` method.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [31]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m \u001b[43mnlp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m iterations \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m15\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/spacy/language.py:1193\u001b[0m, in \u001b[0;36mLanguage.begin_training\u001b[0;34m(self, get_examples, sgd)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbegin_training\u001b[39m(\n\u001b[1;32m   1187\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1188\u001b[0m     get_examples: Optional[Callable[[], Iterable[Example]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1189\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   1190\u001b[0m     sgd: Optional[Optimizer] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1191\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optimizer:\n\u001b[1;32m   1192\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(Warnings\u001b[38;5;241m.\u001b[39mW089, \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m)\n\u001b[0;32m-> 1193\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitialize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_examples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msgd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msgd\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/spacy/language.py:1252\u001b[0m, in \u001b[0;36mLanguage.initialize\u001b[0;34m(self, get_examples, sgd)\u001b[0m\n\u001b[1;32m   1248\u001b[0m         p_settings \u001b[38;5;241m=\u001b[39m I[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomponents\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(name, {})\n\u001b[1;32m   1249\u001b[0m         p_settings \u001b[38;5;241m=\u001b[39m validate_init_settings(\n\u001b[1;32m   1250\u001b[0m             proc\u001b[38;5;241m.\u001b[39minitialize, p_settings, section\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomponents\u001b[39m\u001b[38;5;124m\"\u001b[39m, name\u001b[38;5;241m=\u001b[39mname\n\u001b[1;32m   1251\u001b[0m         )\n\u001b[0;32m-> 1252\u001b[0m         \u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitialize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_examples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnlp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mp_settings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1253\u001b[0m pretrain_cfg \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpretraining\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pretrain_cfg:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/spacy/pipeline/tagger.pyx:271\u001b[0m, in \u001b[0;36mspacy.pipeline.tagger.Tagger.initialize\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/spacy/pipeline/pipe.pyx:104\u001b[0m, in \u001b[0;36mspacy.pipeline.pipe.Pipe._require_labels\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: [E143] Labels for component 'tagger' not initialized. This can be fixed by calling add_label, or by providing a representative batch of examples to the component's `initialize` method."
     ]
    }
   ],
   "source": [
    "optimizer = nlp.begin_training()\n",
    "iterations = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa96936",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-21T22:39:39.359679Z",
     "start_time": "2022-12-21T22:39:39.359667Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from spacy.util import minibatch, compounding\n",
    "from spacy.training import Example\n",
    "\n",
    "for j in range(iterations):\n",
    "    \n",
    "    losses = {}\n",
    "    k = 0\n",
    "    batches = minibatch(train_data, size = compounding(4.,32.,1.001))\n",
    "    \n",
    "    for batch in batches:\n",
    "        text, annotations = zip(*batch)\n",
    "        example = []\n",
    "        for i in range(len(text)):\n",
    "            doc = nlp.make_doc(text[i])\n",
    "            example.append(Example.from_dict(doc, annotations[i]))\n",
    "        nlp.update(example, sgd=optimizer, drop=0.2, losses = losses)\n",
    "        print('Batch No: {} Loss = {}'.format(k, round(losses['textcat_multilabel'])))\n",
    "        k += 1\n",
    "    print(\"\\n\\n Completed Iterations : {} \".format(j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "25404e10",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-21T21:54:06.401586Z",
     "start_time": "2022-12-21T21:54:06.393350Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"Okay so no one's gonna address this? Guess that no one editing this page cares about accuracy.\",\n",
       "  {'cats': {'toxic': 0,\n",
       "    'severe_toxic': 0,\n",
       "    'obscene': 0,\n",
       "    'threat': 0,\n",
       "    'insult': 0,\n",
       "    'identity_hate': 0,\n",
       "    'non_toxic': 1}}),\n",
       " ('\"   In regards to wishful thinking   \"\"Due to Leuchter\\'s ignorance of the large disparity between the amounts of cyanide necessary to kill humans and lice, instead of disproving the homicidal use of gas chambers, the small amounts of cyanide which Leuchter detected actually tended to confirm it.[8]\"\"  It doesn\\'t explain how it tended to confirm it, also this whole sentence is presented as fact like everything else in this biased article. Thus I\\'m assuming it\\'s wishful thinking and only here to discredit Leutcher.   \"',\n",
       "  {'cats': {'toxic': 0,\n",
       "    'severe_toxic': 0,\n",
       "    'obscene': 0,\n",
       "    'threat': 0,\n",
       "    'insult': 0,\n",
       "    'identity_hate': 0,\n",
       "    'non_toxic': 1}}),\n",
       " (\"Ok, so put the pictures somewhere in the article. Beirut cannot be the main picture because it is biased and misportrays events. It makes it appear that Israel planned this attack in the first place. The main picture should show Hezbollah firing missiles because after all Hezbollah started this conflict. If Hezbollah hadn't started it, Israel would NOT have to launch this operation.\",\n",
       "  {'cats': {'toxic': 0,\n",
       "    'severe_toxic': 0,\n",
       "    'obscene': 0,\n",
       "    'threat': 0,\n",
       "    'insult': 0,\n",
       "    'identity_hate': 0,\n",
       "    'non_toxic': 1}}),\n",
       " ('|I apologise for making that remark to Sidaway.  In return, perhaps he could be a little more reasonable in not effectively deleting material from a talk page.',\n",
       "  {'cats': {'toxic': 0,\n",
       "    'severe_toxic': 0,\n",
       "    'obscene': 0,\n",
       "    'threat': 0,\n",
       "    'insult': 0,\n",
       "    'identity_hate': 0,\n",
       "    'non_toxic': 1}}),\n",
       " ('\"  At some point in the article\\'s history, the idea expressed by this passage was followed by some discussion of airflow on both sides of the wing and the point made that some explanations only involve air being deflected by the bottom of the wing. (sometimes called \"\"bullet theroy\"\" or \"\"skipping stone theory\"\").  This was accompanied by a graph showing the pressure distribution along the top and bottom of a wing where the pressure difference along the top was substantially larger than along the bottom.  At some point this was excised, whether by accident or not.  Which is to say that your interpretation of the intent is correct.  I can see where what is actually written may be interpreted differently than what was meant.  A suggestion for how to reword would be welcome.  I\\'ll take a look at earlier versions and see if we edited something out that shouldn\\'t have been.  Personally, I don\\'t read the passage as a statement of cause and effect but I can see how someone might interpret it that way.    \"',\n",
       "  {'cats': {'toxic': 0,\n",
       "    'severe_toxic': 0,\n",
       "    'obscene': 0,\n",
       "    'threat': 0,\n",
       "    'insult': 0,\n",
       "    'identity_hate': 0,\n",
       "    'non_toxic': 1}})]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f0481d57",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-21T21:53:47.982973Z",
     "start_time": "2022-12-21T21:53:47.961913Z"
    }
   },
   "outputs": [],
   "source": [
    "train_comments, test_comments = train_test_split(df[\"comment_text\"],\n",
    "                                                 train_size=0.8,\n",
    "                                                 random_state=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a2853a13",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-21T21:56:18.543100Z",
     "start_time": "2022-12-21T21:55:59.504781Z"
    }
   },
   "outputs": [],
   "source": [
    "# Tokenize the data\n",
    "\n",
    "texts = list(train_comments)\n",
    "docs = [nlp.tokenizer(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a87f5127",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-21T22:36:57.218359Z",
     "start_time": "2022-12-21T22:36:56.721784Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nlp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Use textcat to get the scores for each doc\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m textcat \u001b[38;5;241m=\u001b[39m \u001b[43mnlp\u001b[49m\u001b[38;5;241m.\u001b[39mget_pipe(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtextcat_multilabel\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m scores, _ \u001b[38;5;241m=\u001b[39m textcat\u001b[38;5;241m.\u001b[39mpredict(docs)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(scores[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m5\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nlp' is not defined"
     ]
    }
   ],
   "source": [
    "# Use textcat to get the scores for each doc\n",
    "\n",
    "textcat = nlp.get_pipe('textcat_multilabel')\n",
    "scores, _ = textcat.predict(docs)\n",
    "print(scores[0:5])\n",
    "\n",
    "# The kernel died ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425a2d11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac3dfa1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7ff2f4c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-21T19:49:20.732962Z",
     "start_time": "2022-12-21T19:49:20.262652Z"
    }
   },
   "outputs": [],
   "source": [
    "nlp.to_disk('models/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4fc5aa18",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-21T20:27:52.548418Z",
     "start_time": "2022-12-21T20:27:51.980645Z"
    }
   },
   "outputs": [],
   "source": [
    "np.save('train.npy', train_data, allow_pickle=True)\n",
    "np.save('test.npy', test_data, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b54016",
   "metadata": {},
   "source": [
    "## Results & predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cc43522e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-21T21:15:30.373378Z",
     "start_time": "2022-12-21T21:15:29.980340Z"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = nlp.from_disk('models/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4196be96",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-21T21:15:55.834214Z",
     "start_time": "2022-12-21T21:15:55.819544Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x7f91ddb0d670>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd203f44",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-21T21:07:44.913441Z",
     "start_time": "2022-12-21T21:07:44.345510Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_data = np.load(\"train.npy\", allow_pickle=True)\n",
    "train_data = train_data.tolist()\n",
    "\n",
    "test_data = np.load(\"test.npy\", allow_pickle=True)\n",
    "test_data = test_data.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d28b994",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-21T21:09:43.630342Z",
     "start_time": "2022-12-21T21:09:43.270637Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Argument 'string' has incorrect type (expected str, got list)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[43mmy_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/spacy/language.py:984\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m    965\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m    966\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    967\u001b[0m     text: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    970\u001b[0m     component_cfg: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, Dict[\u001b[38;5;28mstr\u001b[39m, Any]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    971\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Doc:\n\u001b[1;32m    972\u001b[0m     \u001b[38;5;124;03m\"\"\"Apply the pipeline to some text. The text can span multiple sentences,\u001b[39;00m\n\u001b[1;32m    973\u001b[0m \u001b[38;5;124;03m    and can contain arbitrary whitespace. Alignment into the original string\u001b[39;00m\n\u001b[1;32m    974\u001b[0m \u001b[38;5;124;03m    is preserved.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    982\u001b[0m \u001b[38;5;124;03m    DOCS: https://spacy.io/api/language#call\u001b[39;00m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 984\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_doc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    985\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m component_cfg \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    986\u001b[0m         component_cfg \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/spacy/language.py:1066\u001b[0m, in \u001b[0;36mLanguage.make_doc\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1062\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(text) \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_length:\n\u001b[1;32m   1063\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1064\u001b[0m         Errors\u001b[38;5;241m.\u001b[39mE088\u001b[38;5;241m.\u001b[39mformat(length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(text), max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_length)\n\u001b[1;32m   1065\u001b[0m     )\n\u001b[0;32m-> 1066\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: Argument 'string' has incorrect type (expected str, got list)"
     ]
    }
   ],
   "source": [
    "processed = my_model(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e61c55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f5b7e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3e4d2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 734.8438120000001,
   "position": {
    "height": "40px",
    "left": "1307.77px",
    "right": "20px",
    "top": "121px",
    "width": "476.006px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
